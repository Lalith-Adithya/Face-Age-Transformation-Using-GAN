{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyXrHK1rbmGr",
        "outputId": "ff65f167-e77d-4b50-cb40-05d4d4aac1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Data/UTKFace.zip'\n",
        "extract_to = '/content/images'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n"
      ],
      "metadata": {
        "id": "BxD1-vbTbn7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import math\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils"
      ],
      "metadata": {
        "id": "pXoRb37bb5iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration and Hyperparameters\n",
        "\n",
        "img_size = 128\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "lambda_cls = 1\n",
        "lambda_rec = 10\n",
        "\n",
        "# Paths of the datasets and output directories\n",
        "data_root = \"/content/images/UTKFace\"\n",
        "output_dir = \"/content/drive/MyDrive/UTKFace_Outputs\"\n",
        "ckpt_dir   = \"/content/drive/MyDrive/UTKFace_Outputs\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Dkx14-Izb9BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Definition for UTKFace\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = [os.path.join(root_dir, fname)\n",
        "                             for fname in os.listdir(root_dir) if fname.endswith(('.jpg', '.png'))]\n",
        "\n",
        "        self.labels = []\n",
        "        for path in self.image_paths:\n",
        "            fname = os.path.basename(path)\n",
        "            age = int(fname.split('_')[0])\n",
        "            # Map age to group index 0-9\n",
        "            if age < 1:\n",
        "                age = 1\n",
        "            group = min((age - 1) // 10, 9)\n",
        "            self.labels.append(group)\n",
        "        assert len(self.image_paths) == len(self.labels), \"Mismatch in images and labels count\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        age_group = self.labels[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # Create one-hot encoding for the age group label\n",
        "        label_tensor = torch.tensor(age_group, dtype=torch.long)\n",
        "        one_hot = F.one_hot(label_tensor, num_classes=10).float()\n",
        "        return img, one_hot, age_group\n",
        "\n",
        "\n",
        "transform_ops = transforms.Compose([\n",
        "    transforms.CenterCrop(min(img_size, 200)),\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "full_dataset = UTKFaceDataset(data_root, transform=transform_ops)\n",
        "\n",
        "indices = list(range(len(full_dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "split_idx = int(0.9 * len(indices))\n",
        "train_indices = indices[:split_idx]\n",
        "val_indices = indices[split_idx:]\n",
        "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "val_subset = torch.utils.data.Subset(full_dataset, val_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_loader   = DataLoader(val_subset, batch_size=8, shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "epaQnX81cWe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For monitoring, taking a fixed set of sample images from val\n",
        "\n",
        "sample_batch = next(iter(val_loader))\n",
        "sample_images, sample_labels, sample_age_groups = sample_batch\n",
        "\n",
        "if sample_images.shape[0] == 0:\n",
        "    sample_batch = next(iter(train_loader))\n",
        "    sample_images, sample_labels, sample_age_groups = sample_batch\n",
        "\n",
        "sample_images = sample_images[:5]\n",
        "sample_labels = sample_labels[:5]\n",
        "sample_age_groups = sample_age_groups[:5]"
      ],
      "metadata": {
        "id": "or9IyGEzcfD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definitions\n",
        "\n",
        "# Generator Architecture\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_channels=3, label_dim=10, feature_dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_dim = label_dim\n",
        "        in_channels = img_channels + label_dim\n",
        "        nf = feature_dim\n",
        "        self.down1 = nn.Conv2d(in_channels, nf, kernel_size=7, stride=1, padding=3)\n",
        "        self.down2 = nn.Conv2d(nf, nf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.down3 = nn.Conv2d(nf*2, nf*4, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        res_blocks = []\n",
        "        res_channels = nf * 4\n",
        "        for _ in range(6):\n",
        "            res_blocks.append(nn.Sequential(\n",
        "                nn.Conv2d(res_channels, res_channels, kernel_size=3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(res_channels, affine=False),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(res_channels, res_channels, kernel_size=3, stride=1, padding=1),\n",
        "                nn.InstanceNorm2d(res_channels, affine=False)\n",
        "            ))\n",
        "        self.res_blocks = nn.ModuleList(res_blocks)\n",
        "        self.up1 = nn.ConvTranspose2d(res_channels, nf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.up2 = nn.ConvTranspose2d(nf*2, nf, kernel_size=4, stride=2, padding=1)\n",
        "        self.out_conv = nn.Conv2d(nf, img_channels, kernel_size=7, stride=1, padding=3)\n",
        "        self.actvn = nn.ReLU(inplace=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.InstanceNorm2d) or isinstance(m, nn.BatchNorm2d):\n",
        "                if hasattr(m, 'weight') and m.weight is not None:\n",
        "                    nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, img, target_label):\n",
        "        label_maps = target_label[:, :, None, None]\n",
        "        label_maps = label_maps.expand(-1, -1, img.size(2), img.size(3))\n",
        "        x = torch.cat([img, label_maps], dim=1)\n",
        "\n",
        "        x = self.actvn(self.down1(x))\n",
        "        x = self.actvn(self.down2(x))\n",
        "        x = self.actvn(self.down3(x))\n",
        "\n",
        "        for res_block in self.res_blocks:\n",
        "            residual = x\n",
        "            out = res_block(x)\n",
        "            x = self.actvn(out + residual)\n",
        "\n",
        "        x = self.actvn(self.up1(x))\n",
        "        x = self.actvn(self.up2(x))\n",
        "        x = self.tanh(self.out_conv(x))\n",
        "        return x\n",
        "\n",
        "# Discriminator Architecture\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=3, base_features=64, num_classes=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        nf = base_features\n",
        "\n",
        "        self.conv1 = nn.Conv2d(img_channels, nf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(nf, nf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(nf*2, nf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(nf*4, nf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(nf*8, nf*16, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.adv_out = nn.Linear(nf*16, 1)\n",
        "        self.cls_out = nn.Linear(nf*16, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = F.leaky_relu(self.conv1(img), 0.2)\n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv5(x), 0.2)\n",
        "\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        adv_logits = self.adv_out(x)\n",
        "        cls_logits = self.cls_out(x)\n",
        "        return adv_logits, cls_logits"
      ],
      "metadata": {
        "id": "NNQFTc9sciHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate models and optimizers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = Generator(img_channels=3, label_dim=10, feature_dim=64).to(device)\n",
        "D = Discriminator(img_channels=3, base_features=64, num_classes=10).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Loss functions\n",
        "adv_loss_fn = nn.BCEWithLogitsLoss()\n",
        "cls_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Labels for adversarial loss\n",
        "real_label = 1.0\n",
        "fake_label = 0.0"
      ],
      "metadata": {
        "id": "84760pRicmFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1aMTxO3couh",
        "outputId": "3c13a44a-3456-4cd8-8fc9-7e85d8e86205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    G.train()\n",
        "    D.train()\n",
        "    for i, (real_imgs, real_labels, real_age_groups) in enumerate(train_loader):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        real_labels = real_labels.to(device)         # one-hot labels\n",
        "        real_age_groups = real_age_groups.to(device) # numeric labels\n",
        "\n",
        "        #  Train Discriminator\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        adv_logits_real, cls_logits_real = D(real_imgs)\n",
        "\n",
        "        adv_loss_real = adv_loss_fn(adv_logits_real, torch.ones_like(adv_logits_real)*real_label)\n",
        "        cls_loss_real = cls_loss_fn(cls_logits_real, real_age_groups)\n",
        "\n",
        "        target_age_groups = []\n",
        "        for ag in real_age_groups:\n",
        "            tgt = random.randrange(0, 10)\n",
        "            if tgt == ag.item():\n",
        "                tgt = (tgt + random.randrange(1, 10)) % 10\n",
        "            target_age_groups.append(tgt)\n",
        "        target_age_groups = torch.tensor(target_age_groups, dtype=torch.long, device=device)\n",
        "        target_labels = F.one_hot(target_age_groups, num_classes=10).float()\n",
        "        # Generate fake images with target age condition\n",
        "        with torch.no_grad():\n",
        "            fake_imgs = G(real_imgs, target_labels)\n",
        "        # Forward pass fake images through D\n",
        "        adv_logits_fake, _ = D(fake_imgs)\n",
        "        adv_loss_fake = adv_loss_fn(adv_logits_fake, torch.ones_like(adv_logits_fake)*fake_label)\n",
        "        # Total discriminator loss\n",
        "        d_loss = adv_loss_real + adv_loss_fake + lambda_cls * cls_loss_real\n",
        "        # Backprop and optimize D\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        #  Train Generator\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        fake_imgs = G(real_imgs, target_labels)\n",
        "\n",
        "        for p in D.parameters():\n",
        "            p.requires_grad = False\n",
        "        adv_logits_fake, cls_logits_fake = D(fake_imgs)\n",
        "\n",
        "        for p in D.parameters():\n",
        "            p.requires_grad = True\n",
        "        # Generator adversarial loss\n",
        "        g_adv_loss = adv_loss_fn(adv_logits_fake, torch.ones_like(adv_logits_fake)*real_label)\n",
        "        # Generator classification loss\n",
        "        g_cls_loss = cls_loss_fn(cls_logits_fake, target_age_groups)\n",
        "        # Reconstruction cycle loss: reconstruct original image from fake\n",
        "        rec_imgs = G(fake_imgs, real_labels)\n",
        "        rec_loss = F.l1_loss(rec_imgs, real_imgs)\n",
        "        # Total generator loss\n",
        "        g_loss = g_adv_loss + lambda_cls * g_cls_loss + lambda_rec * rec_loss\n",
        "        # Backprop and optimize G\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        if (i+1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {i+1}/{len(train_loader)} \"\n",
        "                  f\"D_loss: {d_loss.item():.3f}  G_loss: {g_loss.item():.3f}  \"\n",
        "                  f\"(Adv: {g_adv_loss.item():.3f}, Cls: {g_cls_loss.item():.3f}, Rec: {rec_loss.item():.3f})\")\n",
        "\n",
        "\n",
        "    # Checking the transformation and saving the checkpoints\n",
        "\n",
        "    G.eval()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save(G.state_dict(), os.path.join(ckpt_dir, f\"generator_epoch_{epoch}.pth\"))\n",
        "        torch.save(D.state_dict(), os.path.join(ckpt_dir, f\"discriminator_epoch_{epoch}.pth\"))\n",
        "        print(f\"Saved checkpoints at epoch {epoch}\")\n",
        "\n",
        "    # Generate and save sample age transformations on fixed sample images\n",
        "    with torch.no_grad():\n",
        "        sample_targets = []\n",
        "        for ag in sample_age_groups:\n",
        "            if ag < 5:\n",
        "                sample_targets.append(random.randint(5, 9))\n",
        "            else:\n",
        "                sample_targets.append(random.randint(0, 4))\n",
        "        sample_targets = torch.tensor(sample_targets, dtype=torch.long, device=device)\n",
        "        sample_target_labels = F.one_hot(sample_targets, num_classes=10).float()\n",
        "        sample_inputs = sample_images.to(device)\n",
        "        gen_samples = G(sample_inputs, sample_target_labels)\n",
        "\n",
        "        all_imgs = []\n",
        "        for idx in range(gen_samples.size(0)):\n",
        "            orig = sample_inputs[idx]\n",
        "            fake = gen_samples[idx]\n",
        "            all_imgs.append(orig.cpu())\n",
        "            all_imgs.append(fake.cpu())\n",
        "\n",
        "        grid = vutils.make_grid(all_imgs, nrow=2, normalize=True, value_range=(-1, 1))\n",
        "        vutils.save_image(grid, os.path.join(output_dir, f\"epoch_{epoch}_comparison.png\"))\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Saved sample comparison image at epoch {epoch}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NskNnzu-cp-Y",
        "outputId": "f57a706f-d0d1-49b8-8f99-3861e844dff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch [1/100] Batch 50/333 D_loss: 3.236  G_loss: 5.198  (Adv: 0.856, Cls: 2.380, Rec: 0.196)\n",
            "Epoch [1/100] Batch 100/333 D_loss: 3.372  G_loss: 5.039  (Adv: 0.842, Cls: 2.554, Rec: 0.164)\n",
            "Epoch [1/100] Batch 150/333 D_loss: 3.057  G_loss: 4.633  (Adv: 0.679, Cls: 2.580, Rec: 0.137)\n",
            "Epoch [1/100] Batch 200/333 D_loss: 3.090  G_loss: 4.755  (Adv: 1.008, Cls: 2.460, Rec: 0.129)\n",
            "Epoch [1/100] Batch 250/333 D_loss: 2.614  G_loss: 4.709  (Adv: 1.188, Cls: 2.297, Rec: 0.122)\n",
            "Epoch [1/100] Batch 300/333 D_loss: 3.047  G_loss: 3.943  (Adv: 0.636, Cls: 2.092, Rec: 0.122)\n",
            "Epoch [2/100] Batch 50/333 D_loss: 2.656  G_loss: 4.005  (Adv: 1.365, Cls: 1.494, Rec: 0.115)\n",
            "Epoch [2/100] Batch 100/333 D_loss: 2.847  G_loss: 4.473  (Adv: 1.586, Cls: 1.548, Rec: 0.134)\n",
            "Epoch [2/100] Batch 150/333 D_loss: 2.723  G_loss: 4.232  (Adv: 1.535, Cls: 1.544, Rec: 0.115)\n",
            "Epoch [2/100] Batch 200/333 D_loss: 2.587  G_loss: 3.839  (Adv: 1.241, Cls: 1.485, Rec: 0.111)\n",
            "Epoch [2/100] Batch 250/333 D_loss: 2.665  G_loss: 3.786  (Adv: 1.419, Cls: 1.270, Rec: 0.110)\n",
            "Epoch [2/100] Batch 300/333 D_loss: 2.813  G_loss: 3.931  (Adv: 1.039, Cls: 1.515, Rec: 0.138)\n",
            "Epoch [3/100] Batch 50/333 D_loss: 2.996  G_loss: 3.600  (Adv: 0.930, Cls: 1.464, Rec: 0.121)\n",
            "Epoch [3/100] Batch 100/333 D_loss: 2.796  G_loss: 3.696  (Adv: 1.011, Cls: 1.547, Rec: 0.114)\n",
            "Epoch [3/100] Batch 150/333 D_loss: 2.867  G_loss: 3.451  (Adv: 1.063, Cls: 1.416, Rec: 0.097)\n",
            "Epoch [3/100] Batch 200/333 D_loss: 2.591  G_loss: 3.474  (Adv: 1.231, Cls: 1.228, Rec: 0.102)\n",
            "Epoch [3/100] Batch 250/333 D_loss: 2.651  G_loss: 3.391  (Adv: 1.100, Cls: 1.324, Rec: 0.097)\n",
            "Epoch [3/100] Batch 300/333 D_loss: 2.770  G_loss: 3.508  (Adv: 0.924, Cls: 1.583, Rec: 0.100)\n",
            "Epoch [4/100] Batch 50/333 D_loss: 2.462  G_loss: 3.626  (Adv: 1.239, Cls: 1.427, Rec: 0.096)\n",
            "Epoch [4/100] Batch 100/333 D_loss: 2.481  G_loss: 3.575  (Adv: 1.290, Cls: 1.341, Rec: 0.094)\n",
            "Epoch [4/100] Batch 150/333 D_loss: 3.468  G_loss: 3.651  (Adv: 1.091, Cls: 1.542, Rec: 0.102)\n",
            "Epoch [4/100] Batch 200/333 D_loss: 2.643  G_loss: 3.591  (Adv: 1.082, Cls: 1.474, Rec: 0.104)\n",
            "Epoch [4/100] Batch 250/333 D_loss: 2.278  G_loss: 3.386  (Adv: 1.368, Cls: 1.111, Rec: 0.091)\n",
            "Epoch [4/100] Batch 300/333 D_loss: 2.548  G_loss: 3.540  (Adv: 1.314, Cls: 1.259, Rec: 0.097)\n",
            "Epoch [5/100] Batch 50/333 D_loss: 2.825  G_loss: 3.298  (Adv: 1.300, Cls: 1.065, Rec: 0.093)\n",
            "Epoch [5/100] Batch 100/333 D_loss: 2.516  G_loss: 3.113  (Adv: 1.156, Cls: 1.146, Rec: 0.081)\n",
            "Epoch [5/100] Batch 150/333 D_loss: 2.622  G_loss: 3.219  (Adv: 1.303, Cls: 1.047, Rec: 0.087)\n",
            "Epoch [5/100] Batch 200/333 D_loss: 2.503  G_loss: 3.053  (Adv: 1.124, Cls: 1.122, Rec: 0.081)\n",
            "Epoch [5/100] Batch 250/333 D_loss: 2.279  G_loss: 3.014  (Adv: 1.088, Cls: 1.058, Rec: 0.087)\n",
            "Epoch [5/100] Batch 300/333 D_loss: 2.532  G_loss: 3.186  (Adv: 1.092, Cls: 1.193, Rec: 0.090)\n",
            "Saved checkpoints at epoch 5\n",
            "Saved sample comparison image at epoch 5\n",
            "Epoch [6/100] Batch 50/333 D_loss: 2.536  G_loss: 3.197  (Adv: 1.329, Cls: 0.938, Rec: 0.093)\n",
            "Epoch [6/100] Batch 100/333 D_loss: 2.345  G_loss: 2.805  (Adv: 1.150, Cls: 0.830, Rec: 0.083)\n",
            "Epoch [6/100] Batch 150/333 D_loss: 2.314  G_loss: 2.963  (Adv: 1.268, Cls: 0.861, Rec: 0.083)\n",
            "Epoch [6/100] Batch 200/333 D_loss: 2.282  G_loss: 3.112  (Adv: 1.301, Cls: 1.045, Rec: 0.077)\n",
            "Epoch [6/100] Batch 250/333 D_loss: 2.105  G_loss: 3.494  (Adv: 1.680, Cls: 0.931, Rec: 0.088)\n",
            "Epoch [6/100] Batch 300/333 D_loss: 2.648  G_loss: 3.275  (Adv: 1.463, Cls: 0.989, Rec: 0.082)\n",
            "Epoch [7/100] Batch 50/333 D_loss: 2.411  G_loss: 3.254  (Adv: 1.510, Cls: 0.925, Rec: 0.082)\n",
            "Epoch [7/100] Batch 100/333 D_loss: 2.608  G_loss: 2.926  (Adv: 1.268, Cls: 0.827, Rec: 0.083)\n",
            "Epoch [7/100] Batch 150/333 D_loss: 2.511  G_loss: 3.102  (Adv: 1.208, Cls: 1.039, Rec: 0.085)\n",
            "Epoch [7/100] Batch 200/333 D_loss: 2.467  G_loss: 3.114  (Adv: 1.428, Cls: 0.780, Rec: 0.091)\n",
            "Epoch [7/100] Batch 250/333 D_loss: 2.343  G_loss: 2.853  (Adv: 1.259, Cls: 0.777, Rec: 0.082)\n",
            "Epoch [7/100] Batch 300/333 D_loss: 2.349  G_loss: 2.957  (Adv: 1.325, Cls: 0.795, Rec: 0.084)\n",
            "Epoch [8/100] Batch 50/333 D_loss: 2.347  G_loss: 2.846  (Adv: 0.952, Cls: 1.056, Rec: 0.084)\n",
            "Epoch [8/100] Batch 100/333 D_loss: 2.062  G_loss: 3.211  (Adv: 1.754, Cls: 0.680, Rec: 0.078)\n",
            "Epoch [8/100] Batch 150/333 D_loss: 2.333  G_loss: 2.767  (Adv: 1.200, Cls: 0.818, Rec: 0.075)\n",
            "Epoch [8/100] Batch 200/333 D_loss: 2.318  G_loss: 2.806  (Adv: 1.305, Cls: 0.720, Rec: 0.078)\n",
            "Epoch [8/100] Batch 250/333 D_loss: 2.435  G_loss: 3.352  (Adv: 1.814, Cls: 0.780, Rec: 0.076)\n",
            "Epoch [8/100] Batch 300/333 D_loss: 2.244  G_loss: 3.037  (Adv: 1.311, Cls: 0.721, Rec: 0.101)\n",
            "Epoch [9/100] Batch 50/333 D_loss: 2.160  G_loss: 2.970  (Adv: 1.357, Cls: 0.775, Rec: 0.084)\n",
            "Epoch [9/100] Batch 100/333 D_loss: 1.886  G_loss: 2.838  (Adv: 1.401, Cls: 0.693, Rec: 0.074)\n",
            "Epoch [9/100] Batch 150/333 D_loss: 2.303  G_loss: 2.969  (Adv: 1.432, Cls: 0.702, Rec: 0.084)\n",
            "Epoch [9/100] Batch 200/333 D_loss: 2.380  G_loss: 2.675  (Adv: 1.126, Cls: 0.689, Rec: 0.086)\n",
            "Epoch [9/100] Batch 250/333 D_loss: 2.336  G_loss: 2.617  (Adv: 1.205, Cls: 0.699, Rec: 0.071)\n",
            "Epoch [9/100] Batch 300/333 D_loss: 2.331  G_loss: 2.577  (Adv: 1.036, Cls: 0.733, Rec: 0.081)\n",
            "Epoch [10/100] Batch 50/333 D_loss: 2.372  G_loss: 2.802  (Adv: 1.104, Cls: 0.976, Rec: 0.072)\n",
            "Epoch [10/100] Batch 100/333 D_loss: 2.298  G_loss: 2.824  (Adv: 1.394, Cls: 0.666, Rec: 0.076)\n",
            "Epoch [10/100] Batch 150/333 D_loss: 2.203  G_loss: 2.809  (Adv: 1.354, Cls: 0.729, Rec: 0.073)\n",
            "Epoch [10/100] Batch 200/333 D_loss: 2.045  G_loss: 2.777  (Adv: 1.418, Cls: 0.639, Rec: 0.072)\n",
            "Epoch [10/100] Batch 250/333 D_loss: 2.319  G_loss: 2.867  (Adv: 1.293, Cls: 0.668, Rec: 0.091)\n",
            "Epoch [10/100] Batch 300/333 D_loss: 2.357  G_loss: 3.055  (Adv: 1.554, Cls: 0.677, Rec: 0.082)\n",
            "Saved checkpoints at epoch 10\n",
            "Saved sample comparison image at epoch 10\n",
            "Epoch [11/100] Batch 50/333 D_loss: 2.155  G_loss: 2.850  (Adv: 1.338, Cls: 0.717, Rec: 0.079)\n",
            "Epoch [11/100] Batch 100/333 D_loss: 2.376  G_loss: 3.100  (Adv: 1.417, Cls: 0.749, Rec: 0.093)\n",
            "Epoch [11/100] Batch 150/333 D_loss: 2.173  G_loss: 2.265  (Adv: 0.933, Cls: 0.594, Rec: 0.074)\n",
            "Epoch [11/100] Batch 200/333 D_loss: 1.973  G_loss: 2.537  (Adv: 1.117, Cls: 0.682, Rec: 0.074)\n",
            "Epoch [11/100] Batch 250/333 D_loss: 2.275  G_loss: 2.655  (Adv: 1.256, Cls: 0.647, Rec: 0.075)\n",
            "Epoch [11/100] Batch 300/333 D_loss: 2.149  G_loss: 2.571  (Adv: 1.001, Cls: 0.842, Rec: 0.073)\n",
            "Epoch [12/100] Batch 50/333 D_loss: 2.354  G_loss: 2.534  (Adv: 1.309, Cls: 0.518, Rec: 0.071)\n",
            "Epoch [12/100] Batch 100/333 D_loss: 2.594  G_loss: 3.012  (Adv: 1.484, Cls: 0.761, Rec: 0.077)\n",
            "Epoch [12/100] Batch 150/333 D_loss: 1.977  G_loss: 2.320  (Adv: 1.082, Cls: 0.498, Rec: 0.074)\n",
            "Epoch [12/100] Batch 200/333 D_loss: 2.347  G_loss: 2.635  (Adv: 1.424, Cls: 0.469, Rec: 0.074)\n",
            "Epoch [12/100] Batch 250/333 D_loss: 2.345  G_loss: 2.711  (Adv: 1.512, Cls: 0.517, Rec: 0.068)\n",
            "Epoch [12/100] Batch 300/333 D_loss: 2.297  G_loss: 2.779  (Adv: 1.426, Cls: 0.618, Rec: 0.073)\n",
            "Epoch [13/100] Batch 50/333 D_loss: 2.128  G_loss: 2.626  (Adv: 1.328, Cls: 0.585, Rec: 0.071)\n",
            "Epoch [13/100] Batch 100/333 D_loss: 2.234  G_loss: 2.673  (Adv: 1.095, Cls: 0.795, Rec: 0.078)\n",
            "Epoch [13/100] Batch 150/333 D_loss: 1.966  G_loss: 2.522  (Adv: 1.298, Cls: 0.534, Rec: 0.069)\n",
            "Epoch [13/100] Batch 200/333 D_loss: 2.006  G_loss: 2.606  (Adv: 1.431, Cls: 0.432, Rec: 0.074)\n",
            "Epoch [13/100] Batch 250/333 D_loss: 1.974  G_loss: 2.449  (Adv: 1.275, Cls: 0.457, Rec: 0.072)\n",
            "Epoch [13/100] Batch 300/333 D_loss: 2.081  G_loss: 2.622  (Adv: 1.420, Cls: 0.456, Rec: 0.075)\n",
            "Epoch [14/100] Batch 50/333 D_loss: 2.281  G_loss: 3.068  (Adv: 1.852, Cls: 0.478, Rec: 0.074)\n",
            "Epoch [14/100] Batch 100/333 D_loss: 2.332  G_loss: 2.503  (Adv: 0.985, Cls: 0.591, Rec: 0.093)\n",
            "Epoch [14/100] Batch 150/333 D_loss: 2.112  G_loss: 2.747  (Adv: 1.454, Cls: 0.486, Rec: 0.081)\n",
            "Epoch [14/100] Batch 200/333 D_loss: 2.294  G_loss: 2.652  (Adv: 1.136, Cls: 0.826, Rec: 0.069)\n",
            "Epoch [14/100] Batch 250/333 D_loss: 2.305  G_loss: 2.470  (Adv: 1.300, Cls: 0.414, Rec: 0.076)\n",
            "Epoch [14/100] Batch 300/333 D_loss: 2.032  G_loss: 2.440  (Adv: 1.228, Cls: 0.454, Rec: 0.076)\n",
            "Epoch [15/100] Batch 50/333 D_loss: 2.091  G_loss: 2.852  (Adv: 1.557, Cls: 0.396, Rec: 0.090)\n",
            "Epoch [15/100] Batch 100/333 D_loss: 2.399  G_loss: 2.766  (Adv: 1.571, Cls: 0.532, Rec: 0.066)\n",
            "Epoch [15/100] Batch 150/333 D_loss: 2.279  G_loss: 2.529  (Adv: 1.359, Cls: 0.421, Rec: 0.075)\n",
            "Epoch [15/100] Batch 200/333 D_loss: 2.109  G_loss: 2.563  (Adv: 1.273, Cls: 0.620, Rec: 0.067)\n",
            "Epoch [15/100] Batch 250/333 D_loss: 2.264  G_loss: 2.736  (Adv: 1.606, Cls: 0.400, Rec: 0.073)\n",
            "Epoch [15/100] Batch 300/333 D_loss: 2.143  G_loss: 2.293  (Adv: 1.211, Cls: 0.405, Rec: 0.068)\n",
            "Saved checkpoints at epoch 15\n",
            "Saved sample comparison image at epoch 15\n",
            "Epoch [16/100] Batch 50/333 D_loss: 1.980  G_loss: 2.353  (Adv: 1.245, Cls: 0.410, Rec: 0.070)\n",
            "Epoch [16/100] Batch 100/333 D_loss: 2.008  G_loss: 2.516  (Adv: 1.244, Cls: 0.585, Rec: 0.069)\n",
            "Epoch [16/100] Batch 150/333 D_loss: 2.203  G_loss: 2.423  (Adv: 1.275, Cls: 0.424, Rec: 0.072)\n",
            "Epoch [16/100] Batch 200/333 D_loss: 1.830  G_loss: 2.487  (Adv: 1.604, Cls: 0.198, Rec: 0.069)\n",
            "Epoch [16/100] Batch 250/333 D_loss: 2.305  G_loss: 2.990  (Adv: 1.904, Cls: 0.388, Rec: 0.070)\n",
            "Epoch [16/100] Batch 300/333 D_loss: 1.851  G_loss: 2.725  (Adv: 1.516, Cls: 0.392, Rec: 0.082)\n",
            "Epoch [17/100] Batch 50/333 D_loss: 1.838  G_loss: 2.318  (Adv: 1.286, Cls: 0.359, Rec: 0.067)\n",
            "Epoch [17/100] Batch 100/333 D_loss: 2.218  G_loss: 2.677  (Adv: 1.531, Cls: 0.448, Rec: 0.070)\n",
            "Epoch [17/100] Batch 150/333 D_loss: 1.997  G_loss: 2.311  (Adv: 1.214, Cls: 0.404, Rec: 0.069)\n",
            "Epoch [17/100] Batch 200/333 D_loss: 1.965  G_loss: 2.372  (Adv: 1.279, Cls: 0.398, Rec: 0.069)\n",
            "Epoch [17/100] Batch 250/333 D_loss: 2.118  G_loss: 2.806  (Adv: 1.579, Cls: 0.520, Rec: 0.071)\n",
            "Epoch [17/100] Batch 300/333 D_loss: 2.131  G_loss: 2.605  (Adv: 1.434, Cls: 0.434, Rec: 0.074)\n",
            "Epoch [18/100] Batch 50/333 D_loss: 2.143  G_loss: 2.783  (Adv: 1.559, Cls: 0.382, Rec: 0.084)\n",
            "Epoch [18/100] Batch 100/333 D_loss: 2.189  G_loss: 2.575  (Adv: 1.530, Cls: 0.278, Rec: 0.077)\n",
            "Epoch [18/100] Batch 150/333 D_loss: 1.650  G_loss: 2.681  (Adv: 1.352, Cls: 0.602, Rec: 0.073)\n",
            "Epoch [18/100] Batch 200/333 D_loss: 1.593  G_loss: 2.554  (Adv: 1.390, Cls: 0.454, Rec: 0.071)\n",
            "Epoch [18/100] Batch 250/333 D_loss: 1.950  G_loss: 2.430  (Adv: 1.430, Cls: 0.341, Rec: 0.066)\n",
            "Epoch [18/100] Batch 300/333 D_loss: 1.975  G_loss: 2.567  (Adv: 1.285, Cls: 0.577, Rec: 0.071)\n",
            "Epoch [19/100] Batch 50/333 D_loss: 1.912  G_loss: 2.542  (Adv: 1.481, Cls: 0.278, Rec: 0.078)\n",
            "Epoch [19/100] Batch 100/333 D_loss: 2.049  G_loss: 2.281  (Adv: 1.205, Cls: 0.384, Rec: 0.069)\n",
            "Epoch [19/100] Batch 150/333 D_loss: 1.722  G_loss: 2.545  (Adv: 1.439, Cls: 0.402, Rec: 0.070)\n",
            "Epoch [19/100] Batch 200/333 D_loss: 1.926  G_loss: 2.610  (Adv: 1.653, Cls: 0.265, Rec: 0.069)\n",
            "Epoch [19/100] Batch 250/333 D_loss: 1.938  G_loss: 2.457  (Adv: 1.394, Cls: 0.400, Rec: 0.066)\n",
            "Epoch [19/100] Batch 300/333 D_loss: 2.187  G_loss: 1.948  (Adv: 1.009, Cls: 0.272, Rec: 0.067)\n",
            "Epoch [20/100] Batch 50/333 D_loss: 1.886  G_loss: 2.129  (Adv: 1.003, Cls: 0.302, Rec: 0.082)\n",
            "Epoch [20/100] Batch 100/333 D_loss: 1.915  G_loss: 2.526  (Adv: 1.304, Cls: 0.361, Rec: 0.086)\n",
            "Epoch [20/100] Batch 150/333 D_loss: 1.896  G_loss: 2.394  (Adv: 1.478, Cls: 0.213, Rec: 0.070)\n",
            "Epoch [20/100] Batch 200/333 D_loss: 2.830  G_loss: 2.644  (Adv: 1.227, Cls: 0.712, Rec: 0.070)\n",
            "Epoch [20/100] Batch 250/333 D_loss: 1.697  G_loss: 2.664  (Adv: 1.588, Cls: 0.395, Rec: 0.068)\n",
            "Epoch [20/100] Batch 300/333 D_loss: 1.988  G_loss: 2.648  (Adv: 1.186, Cls: 0.720, Rec: 0.074)\n",
            "Saved checkpoints at epoch 20\n",
            "Saved sample comparison image at epoch 20\n",
            "Epoch [21/100] Batch 50/333 D_loss: 1.633  G_loss: 2.327  (Adv: 1.308, Cls: 0.335, Rec: 0.068)\n",
            "Epoch [21/100] Batch 100/333 D_loss: 2.154  G_loss: 3.437  (Adv: 2.433, Cls: 0.293, Rec: 0.071)\n",
            "Epoch [21/100] Batch 150/333 D_loss: 1.910  G_loss: 2.584  (Adv: 1.483, Cls: 0.379, Rec: 0.072)\n",
            "Epoch [21/100] Batch 200/333 D_loss: 1.691  G_loss: 2.957  (Adv: 1.802, Cls: 0.440, Rec: 0.072)\n",
            "Epoch [21/100] Batch 250/333 D_loss: 1.857  G_loss: 2.715  (Adv: 1.749, Cls: 0.260, Rec: 0.071)\n",
            "Epoch [21/100] Batch 300/333 D_loss: 1.796  G_loss: 2.711  (Adv: 1.702, Cls: 0.301, Rec: 0.071)\n",
            "Epoch [22/100] Batch 50/333 D_loss: 1.545  G_loss: 2.439  (Adv: 1.389, Cls: 0.389, Rec: 0.066)\n",
            "Epoch [22/100] Batch 100/333 D_loss: 2.050  G_loss: 2.660  (Adv: 1.668, Cls: 0.233, Rec: 0.076)\n",
            "Epoch [22/100] Batch 150/333 D_loss: 1.726  G_loss: 2.705  (Adv: 1.480, Cls: 0.522, Rec: 0.070)\n",
            "Epoch [22/100] Batch 200/333 D_loss: 1.884  G_loss: 2.486  (Adv: 1.398, Cls: 0.299, Rec: 0.079)\n",
            "Epoch [22/100] Batch 250/333 D_loss: 1.830  G_loss: 2.449  (Adv: 1.381, Cls: 0.376, Rec: 0.069)\n",
            "Epoch [22/100] Batch 300/333 D_loss: 1.532  G_loss: 2.532  (Adv: 1.458, Cls: 0.318, Rec: 0.076)\n",
            "Epoch [23/100] Batch 50/333 D_loss: 1.454  G_loss: 2.756  (Adv: 1.567, Cls: 0.467, Rec: 0.072)\n",
            "Epoch [23/100] Batch 100/333 D_loss: 1.870  G_loss: 2.213  (Adv: 1.207, Cls: 0.288, Rec: 0.072)\n",
            "Epoch [23/100] Batch 150/333 D_loss: 1.829  G_loss: 2.279  (Adv: 1.104, Cls: 0.462, Rec: 0.071)\n",
            "Epoch [23/100] Batch 200/333 D_loss: 1.450  G_loss: 2.609  (Adv: 1.508, Cls: 0.338, Rec: 0.076)\n",
            "Epoch [23/100] Batch 250/333 D_loss: 1.828  G_loss: 2.528  (Adv: 1.421, Cls: 0.319, Rec: 0.079)\n",
            "Epoch [23/100] Batch 300/333 D_loss: 1.755  G_loss: 2.766  (Adv: 1.753, Cls: 0.183, Rec: 0.083)\n",
            "Epoch [24/100] Batch 50/333 D_loss: 1.818  G_loss: 2.736  (Adv: 1.700, Cls: 0.301, Rec: 0.073)\n",
            "Epoch [24/100] Batch 100/333 D_loss: 1.789  G_loss: 2.134  (Adv: 1.016, Cls: 0.390, Rec: 0.073)\n",
            "Epoch [24/100] Batch 150/333 D_loss: 1.547  G_loss: 2.613  (Adv: 1.670, Cls: 0.191, Rec: 0.075)\n",
            "Epoch [24/100] Batch 200/333 D_loss: 2.353  G_loss: 2.704  (Adv: 1.453, Cls: 0.471, Rec: 0.078)\n",
            "Epoch [24/100] Batch 250/333 D_loss: 1.853  G_loss: 2.829  (Adv: 1.268, Cls: 0.749, Rec: 0.081)\n",
            "Epoch [24/100] Batch 300/333 D_loss: 1.659  G_loss: 3.184  (Adv: 2.131, Cls: 0.171, Rec: 0.088)\n",
            "Epoch [25/100] Batch 50/333 D_loss: 1.414  G_loss: 2.588  (Adv: 1.656, Cls: 0.221, Rec: 0.071)\n",
            "Epoch [25/100] Batch 100/333 D_loss: 1.999  G_loss: 3.043  (Adv: 2.088, Cls: 0.257, Rec: 0.070)\n",
            "Epoch [25/100] Batch 150/333 D_loss: 1.242  G_loss: 2.227  (Adv: 1.380, Cls: 0.146, Rec: 0.070)\n",
            "Epoch [25/100] Batch 200/333 D_loss: 1.555  G_loss: 2.699  (Adv: 1.667, Cls: 0.293, Rec: 0.074)\n",
            "Epoch [25/100] Batch 250/333 D_loss: 1.759  G_loss: 3.697  (Adv: 2.603, Cls: 0.337, Rec: 0.076)\n",
            "Epoch [25/100] Batch 300/333 D_loss: 1.571  G_loss: 2.418  (Adv: 1.313, Cls: 0.378, Rec: 0.073)\n",
            "Saved checkpoints at epoch 25\n",
            "Saved sample comparison image at epoch 25\n",
            "Epoch [26/100] Batch 50/333 D_loss: 1.680  G_loss: 2.528  (Adv: 1.491, Cls: 0.241, Rec: 0.080)\n",
            "Epoch [26/100] Batch 100/333 D_loss: 2.014  G_loss: 2.153  (Adv: 1.063, Cls: 0.351, Rec: 0.074)\n",
            "Epoch [26/100] Batch 150/333 D_loss: 1.618  G_loss: 2.781  (Adv: 1.921, Cls: 0.193, Rec: 0.067)\n",
            "Epoch [26/100] Batch 200/333 D_loss: 1.449  G_loss: 2.044  (Adv: 1.220, Cls: 0.109, Rec: 0.072)\n",
            "Epoch [26/100] Batch 250/333 D_loss: 1.481  G_loss: 3.066  (Adv: 1.850, Cls: 0.455, Rec: 0.076)\n",
            "Epoch [26/100] Batch 300/333 D_loss: 1.497  G_loss: 2.549  (Adv: 1.416, Cls: 0.303, Rec: 0.083)\n",
            "Epoch [27/100] Batch 50/333 D_loss: 1.347  G_loss: 2.595  (Adv: 1.464, Cls: 0.380, Rec: 0.075)\n",
            "Epoch [27/100] Batch 100/333 D_loss: 1.363  G_loss: 2.927  (Adv: 1.590, Cls: 0.637, Rec: 0.070)\n",
            "Epoch [27/100] Batch 150/333 D_loss: 1.240  G_loss: 2.818  (Adv: 1.739, Cls: 0.305, Rec: 0.077)\n",
            "Epoch [27/100] Batch 200/333 D_loss: 1.467  G_loss: 2.553  (Adv: 1.526, Cls: 0.315, Rec: 0.071)\n",
            "Epoch [27/100] Batch 250/333 D_loss: 1.537  G_loss: 2.204  (Adv: 1.186, Cls: 0.263, Rec: 0.075)\n",
            "Epoch [27/100] Batch 300/333 D_loss: 1.581  G_loss: 2.689  (Adv: 1.690, Cls: 0.345, Rec: 0.065)\n",
            "Epoch [28/100] Batch 50/333 D_loss: 1.238  G_loss: 2.432  (Adv: 1.263, Cls: 0.376, Rec: 0.079)\n",
            "Epoch [28/100] Batch 100/333 D_loss: 1.337  G_loss: 2.509  (Adv: 1.488, Cls: 0.337, Rec: 0.068)\n",
            "Epoch [28/100] Batch 150/333 D_loss: 1.285  G_loss: 2.371  (Adv: 1.374, Cls: 0.305, Rec: 0.069)\n",
            "Epoch [28/100] Batch 200/333 D_loss: 1.397  G_loss: 3.033  (Adv: 2.144, Cls: 0.144, Rec: 0.074)\n",
            "Epoch [28/100] Batch 250/333 D_loss: 1.261  G_loss: 2.810  (Adv: 1.654, Cls: 0.482, Rec: 0.067)\n",
            "Epoch [28/100] Batch 300/333 D_loss: 1.435  G_loss: 2.537  (Adv: 1.556, Cls: 0.277, Rec: 0.070)\n",
            "Epoch [29/100] Batch 50/333 D_loss: 1.643  G_loss: 2.390  (Adv: 1.144, Cls: 0.465, Rec: 0.078)\n",
            "Epoch [29/100] Batch 100/333 D_loss: 1.234  G_loss: 3.404  (Adv: 1.807, Cls: 0.683, Rec: 0.091)\n",
            "Epoch [29/100] Batch 150/333 D_loss: 1.179  G_loss: 2.747  (Adv: 1.813, Cls: 0.181, Rec: 0.075)\n",
            "Epoch [29/100] Batch 200/333 D_loss: 1.373  G_loss: 2.577  (Adv: 1.557, Cls: 0.171, Rec: 0.085)\n",
            "Epoch [29/100] Batch 250/333 D_loss: 1.332  G_loss: 2.724  (Adv: 1.652, Cls: 0.317, Rec: 0.075)\n",
            "Epoch [29/100] Batch 300/333 D_loss: 1.136  G_loss: 2.378  (Adv: 1.499, Cls: 0.167, Rec: 0.071)\n",
            "Epoch [30/100] Batch 50/333 D_loss: 1.336  G_loss: 2.668  (Adv: 1.737, Cls: 0.170, Rec: 0.076)\n",
            "Epoch [30/100] Batch 100/333 D_loss: 1.212  G_loss: 2.347  (Adv: 1.342, Cls: 0.300, Rec: 0.071)\n",
            "Epoch [30/100] Batch 150/333 D_loss: 1.262  G_loss: 2.647  (Adv: 1.386, Cls: 0.479, Rec: 0.078)\n",
            "Epoch [30/100] Batch 200/333 D_loss: 1.572  G_loss: 2.655  (Adv: 1.795, Cls: 0.066, Rec: 0.079)\n",
            "Epoch [30/100] Batch 250/333 D_loss: 1.268  G_loss: 2.615  (Adv: 1.610, Cls: 0.268, Rec: 0.074)\n",
            "Epoch [30/100] Batch 300/333 D_loss: 1.316  G_loss: 2.300  (Adv: 1.512, Cls: 0.099, Rec: 0.069)\n",
            "Saved checkpoints at epoch 30\n",
            "Saved sample comparison image at epoch 30\n",
            "Epoch [31/100] Batch 50/333 D_loss: 1.222  G_loss: 2.656  (Adv: 1.552, Cls: 0.388, Rec: 0.072)\n",
            "Epoch [31/100] Batch 100/333 D_loss: 2.024  G_loss: 2.772  (Adv: 1.383, Cls: 0.612, Rec: 0.078)\n",
            "Epoch [31/100] Batch 150/333 D_loss: 1.316  G_loss: 2.935  (Adv: 1.806, Cls: 0.443, Rec: 0.069)\n",
            "Epoch [31/100] Batch 200/333 D_loss: 1.368  G_loss: 2.372  (Adv: 1.346, Cls: 0.271, Rec: 0.076)\n",
            "Epoch [31/100] Batch 250/333 D_loss: 1.195  G_loss: 2.581  (Adv: 1.720, Cls: 0.168, Rec: 0.069)\n",
            "Epoch [31/100] Batch 300/333 D_loss: 1.277  G_loss: 2.670  (Adv: 1.799, Cls: 0.160, Rec: 0.071)\n",
            "Epoch [32/100] Batch 50/333 D_loss: 1.171  G_loss: 2.292  (Adv: 1.469, Cls: 0.129, Rec: 0.069)\n",
            "Epoch [32/100] Batch 100/333 D_loss: 1.016  G_loss: 3.002  (Adv: 2.126, Cls: 0.155, Rec: 0.072)\n",
            "Epoch [32/100] Batch 150/333 D_loss: 1.362  G_loss: 2.677  (Adv: 1.548, Cls: 0.348, Rec: 0.078)\n",
            "Epoch [32/100] Batch 200/333 D_loss: 1.136  G_loss: 2.888  (Adv: 2.015, Cls: 0.080, Rec: 0.079)\n",
            "Epoch [32/100] Batch 250/333 D_loss: 1.181  G_loss: 2.624  (Adv: 1.592, Cls: 0.302, Rec: 0.073)\n",
            "Epoch [32/100] Batch 300/333 D_loss: 1.246  G_loss: 2.999  (Adv: 2.033, Cls: 0.212, Rec: 0.075)\n",
            "Epoch [33/100] Batch 50/333 D_loss: 1.307  G_loss: 2.705  (Adv: 1.772, Cls: 0.191, Rec: 0.074)\n",
            "Epoch [33/100] Batch 100/333 D_loss: 1.015  G_loss: 2.855  (Adv: 2.051, Cls: 0.096, Rec: 0.071)\n",
            "Epoch [33/100] Batch 150/333 D_loss: 1.091  G_loss: 2.623  (Adv: 1.641, Cls: 0.248, Rec: 0.073)\n",
            "Epoch [33/100] Batch 200/333 D_loss: 1.054  G_loss: 2.847  (Adv: 1.922, Cls: 0.149, Rec: 0.078)\n",
            "Epoch [33/100] Batch 250/333 D_loss: 1.349  G_loss: 2.642  (Adv: 1.516, Cls: 0.385, Rec: 0.074)\n",
            "Epoch [33/100] Batch 300/333 D_loss: 1.077  G_loss: 2.508  (Adv: 1.447, Cls: 0.333, Rec: 0.073)\n",
            "Epoch [34/100] Batch 50/333 D_loss: 0.988  G_loss: 2.504  (Adv: 1.637, Cls: 0.078, Rec: 0.079)\n",
            "Epoch [34/100] Batch 100/333 D_loss: 1.207  G_loss: 2.335  (Adv: 1.357, Cls: 0.263, Rec: 0.071)\n",
            "Epoch [34/100] Batch 150/333 D_loss: 1.091  G_loss: 2.305  (Adv: 1.466, Cls: 0.154, Rec: 0.069)\n",
            "Epoch [34/100] Batch 200/333 D_loss: 0.944  G_loss: 2.579  (Adv: 1.690, Cls: 0.133, Rec: 0.076)\n",
            "Epoch [34/100] Batch 250/333 D_loss: 1.073  G_loss: 2.650  (Adv: 1.891, Cls: 0.096, Rec: 0.066)\n",
            "Epoch [34/100] Batch 300/333 D_loss: 1.247  G_loss: 2.622  (Adv: 1.677, Cls: 0.179, Rec: 0.077)\n",
            "Epoch [35/100] Batch 50/333 D_loss: 1.419  G_loss: 2.013  (Adv: 1.157, Cls: 0.120, Rec: 0.074)\n",
            "Epoch [35/100] Batch 100/333 D_loss: 1.377  G_loss: 3.009  (Adv: 1.710, Cls: 0.563, Rec: 0.074)\n",
            "Epoch [35/100] Batch 150/333 D_loss: 0.890  G_loss: 2.704  (Adv: 1.792, Cls: 0.163, Rec: 0.075)\n",
            "Epoch [35/100] Batch 200/333 D_loss: 1.185  G_loss: 2.609  (Adv: 1.511, Cls: 0.334, Rec: 0.076)\n",
            "Epoch [35/100] Batch 250/333 D_loss: 1.355  G_loss: 3.391  (Adv: 2.472, Cls: 0.150, Rec: 0.077)\n",
            "Epoch [35/100] Batch 300/333 D_loss: 1.138  G_loss: 2.479  (Adv: 1.688, Cls: 0.088, Rec: 0.070)\n",
            "Saved checkpoints at epoch 35\n",
            "Saved sample comparison image at epoch 35\n",
            "Epoch [36/100] Batch 50/333 D_loss: 1.227  G_loss: 2.524  (Adv: 1.396, Cls: 0.250, Rec: 0.088)\n",
            "Epoch [36/100] Batch 100/333 D_loss: 1.096  G_loss: 2.545  (Adv: 1.642, Cls: 0.223, Rec: 0.068)\n",
            "Epoch [36/100] Batch 150/333 D_loss: 1.140  G_loss: 2.786  (Adv: 1.690, Cls: 0.375, Rec: 0.072)\n",
            "Epoch [36/100] Batch 200/333 D_loss: 1.035  G_loss: 2.166  (Adv: 1.348, Cls: 0.118, Rec: 0.070)\n",
            "Epoch [36/100] Batch 250/333 D_loss: 1.021  G_loss: 2.842  (Adv: 1.993, Cls: 0.121, Rec: 0.073)\n",
            "Epoch [36/100] Batch 300/333 D_loss: 1.124  G_loss: 2.126  (Adv: 1.351, Cls: 0.036, Rec: 0.074)\n",
            "Epoch [37/100] Batch 50/333 D_loss: 1.103  G_loss: 4.626  (Adv: 3.494, Cls: 0.335, Rec: 0.080)\n",
            "Epoch [37/100] Batch 100/333 D_loss: 1.115  G_loss: 3.236  (Adv: 2.392, Cls: 0.112, Rec: 0.073)\n",
            "Epoch [37/100] Batch 150/333 D_loss: 1.067  G_loss: 2.885  (Adv: 1.836, Cls: 0.297, Rec: 0.075)\n",
            "Epoch [37/100] Batch 200/333 D_loss: 0.865  G_loss: 3.248  (Adv: 2.128, Cls: 0.266, Rec: 0.085)\n",
            "Epoch [37/100] Batch 250/333 D_loss: 1.010  G_loss: 3.219  (Adv: 2.217, Cls: 0.278, Rec: 0.072)\n",
            "Epoch [37/100] Batch 300/333 D_loss: 1.307  G_loss: 4.041  (Adv: 3.070, Cls: 0.183, Rec: 0.079)\n",
            "Epoch [38/100] Batch 50/333 D_loss: 1.221  G_loss: 2.341  (Adv: 1.308, Cls: 0.289, Rec: 0.074)\n",
            "Epoch [38/100] Batch 100/333 D_loss: 1.185  G_loss: 2.994  (Adv: 1.565, Cls: 0.650, Rec: 0.078)\n",
            "Epoch [38/100] Batch 150/333 D_loss: 1.378  G_loss: 3.776  (Adv: 2.362, Cls: 0.633, Rec: 0.078)\n",
            "Epoch [38/100] Batch 200/333 D_loss: 0.903  G_loss: 2.252  (Adv: 1.426, Cls: 0.093, Rec: 0.073)\n",
            "Epoch [38/100] Batch 250/333 D_loss: 1.242  G_loss: 2.732  (Adv: 1.706, Cls: 0.317, Rec: 0.071)\n",
            "Epoch [38/100] Batch 300/333 D_loss: 1.057  G_loss: 3.076  (Adv: 1.792, Cls: 0.384, Rec: 0.090)\n",
            "Epoch [39/100] Batch 50/333 D_loss: 1.066  G_loss: 2.339  (Adv: 1.402, Cls: 0.113, Rec: 0.082)\n",
            "Epoch [39/100] Batch 100/333 D_loss: 1.768  G_loss: 3.219  (Adv: 2.153, Cls: 0.212, Rec: 0.085)\n",
            "Epoch [39/100] Batch 150/333 D_loss: 1.126  G_loss: 2.353  (Adv: 1.347, Cls: 0.279, Rec: 0.073)\n",
            "Epoch [39/100] Batch 200/333 D_loss: 1.089  G_loss: 2.743  (Adv: 1.806, Cls: 0.182, Rec: 0.075)\n",
            "Epoch [39/100] Batch 250/333 D_loss: 1.022  G_loss: 2.498  (Adv: 1.469, Cls: 0.207, Rec: 0.082)\n",
            "Epoch [39/100] Batch 300/333 D_loss: 0.814  G_loss: 2.798  (Adv: 1.865, Cls: 0.098, Rec: 0.083)\n",
            "Epoch [40/100] Batch 50/333 D_loss: 0.800  G_loss: 3.516  (Adv: 2.603, Cls: 0.180, Rec: 0.073)\n",
            "Epoch [40/100] Batch 100/333 D_loss: 0.869  G_loss: 3.976  (Adv: 2.802, Cls: 0.343, Rec: 0.083)\n",
            "Epoch [40/100] Batch 150/333 D_loss: 1.442  G_loss: 2.898  (Adv: 1.540, Cls: 0.605, Rec: 0.075)\n",
            "Epoch [40/100] Batch 200/333 D_loss: 1.298  G_loss: 3.663  (Adv: 1.991, Cls: 0.778, Rec: 0.089)\n",
            "Epoch [40/100] Batch 250/333 D_loss: 1.255  G_loss: 2.725  (Adv: 1.330, Cls: 0.520, Rec: 0.088)\n",
            "Epoch [40/100] Batch 300/333 D_loss: 1.243  G_loss: 2.398  (Adv: 0.995, Cls: 0.595, Rec: 0.081)\n",
            "Saved checkpoints at epoch 40\n",
            "Saved sample comparison image at epoch 40\n",
            "Epoch [41/100] Batch 50/333 D_loss: 1.049  G_loss: 2.810  (Adv: 1.940, Cls: 0.113, Rec: 0.076)\n",
            "Epoch [41/100] Batch 100/333 D_loss: 1.071  G_loss: 3.707  (Adv: 2.602, Cls: 0.240, Rec: 0.086)\n",
            "Epoch [41/100] Batch 150/333 D_loss: 1.108  G_loss: 3.746  (Adv: 2.339, Cls: 0.563, Rec: 0.084)\n",
            "Epoch [41/100] Batch 200/333 D_loss: 1.111  G_loss: 3.853  (Adv: 2.695, Cls: 0.374, Rec: 0.078)\n",
            "Epoch [41/100] Batch 250/333 D_loss: 1.674  G_loss: 2.881  (Adv: 1.723, Cls: 0.391, Rec: 0.077)\n",
            "Epoch [41/100] Batch 300/333 D_loss: 0.837  G_loss: 2.968  (Adv: 1.945, Cls: 0.166, Rec: 0.086)\n",
            "Epoch [42/100] Batch 50/333 D_loss: 1.226  G_loss: 2.727  (Adv: 1.671, Cls: 0.201, Rec: 0.085)\n",
            "Epoch [42/100] Batch 100/333 D_loss: 0.873  G_loss: 2.916  (Adv: 2.001, Cls: 0.150, Rec: 0.077)\n",
            "Epoch [42/100] Batch 150/333 D_loss: 1.072  G_loss: 3.559  (Adv: 2.454, Cls: 0.348, Rec: 0.076)\n",
            "Epoch [42/100] Batch 200/333 D_loss: 1.115  G_loss: 3.206  (Adv: 1.880, Cls: 0.573, Rec: 0.075)\n",
            "Epoch [42/100] Batch 250/333 D_loss: 0.897  G_loss: 2.803  (Adv: 1.616, Cls: 0.367, Rec: 0.082)\n",
            "Epoch [42/100] Batch 300/333 D_loss: 0.925  G_loss: 3.004  (Adv: 2.057, Cls: 0.139, Rec: 0.081)\n",
            "Epoch [43/100] Batch 50/333 D_loss: 0.934  G_loss: 3.933  (Adv: 2.987, Cls: 0.167, Rec: 0.078)\n",
            "Epoch [43/100] Batch 100/333 D_loss: 1.008  G_loss: 3.401  (Adv: 2.330, Cls: 0.251, Rec: 0.082)\n",
            "Epoch [43/100] Batch 150/333 D_loss: 0.773  G_loss: 2.733  (Adv: 1.863, Cls: 0.085, Rec: 0.078)\n",
            "Epoch [43/100] Batch 200/333 D_loss: 1.052  G_loss: 3.852  (Adv: 2.602, Cls: 0.420, Rec: 0.083)\n",
            "Epoch [43/100] Batch 250/333 D_loss: 1.015  G_loss: 4.162  (Adv: 3.133, Cls: 0.266, Rec: 0.076)\n",
            "Epoch [43/100] Batch 300/333 D_loss: 1.301  G_loss: 3.288  (Adv: 1.859, Cls: 0.620, Rec: 0.081)\n",
            "Epoch [44/100] Batch 50/333 D_loss: 1.074  G_loss: 3.570  (Adv: 2.470, Cls: 0.308, Rec: 0.079)\n",
            "Epoch [44/100] Batch 100/333 D_loss: 1.186  G_loss: 2.506  (Adv: 1.615, Cls: 0.137, Rec: 0.075)\n",
            "Epoch [44/100] Batch 150/333 D_loss: 0.839  G_loss: 3.809  (Adv: 2.968, Cls: 0.034, Rec: 0.081)\n",
            "Epoch [44/100] Batch 200/333 D_loss: 0.926  G_loss: 3.081  (Adv: 2.218, Cls: 0.024, Rec: 0.084)\n",
            "Epoch [44/100] Batch 250/333 D_loss: 1.008  G_loss: 2.753  (Adv: 1.753, Cls: 0.246, Rec: 0.075)\n",
            "Epoch [44/100] Batch 300/333 D_loss: 1.136  G_loss: 4.449  (Adv: 3.540, Cls: 0.177, Rec: 0.073)\n",
            "Epoch [45/100] Batch 50/333 D_loss: 0.781  G_loss: 2.774  (Adv: 1.778, Cls: 0.259, Rec: 0.074)\n",
            "Epoch [45/100] Batch 100/333 D_loss: 0.980  G_loss: 3.627  (Adv: 2.511, Cls: 0.320, Rec: 0.080)\n",
            "Epoch [45/100] Batch 150/333 D_loss: 1.099  G_loss: 3.389  (Adv: 2.627, Cls: 0.034, Rec: 0.073)\n",
            "Epoch [45/100] Batch 200/333 D_loss: 1.045  G_loss: 2.689  (Adv: 1.684, Cls: 0.281, Rec: 0.072)\n",
            "Epoch [45/100] Batch 250/333 D_loss: 0.930  G_loss: 2.587  (Adv: 1.621, Cls: 0.231, Rec: 0.074)\n",
            "Epoch [45/100] Batch 300/333 D_loss: 0.974  G_loss: 3.565  (Adv: 2.524, Cls: 0.228, Rec: 0.081)\n",
            "Saved checkpoints at epoch 45\n",
            "Saved sample comparison image at epoch 45\n",
            "Epoch [46/100] Batch 50/333 D_loss: 1.034  G_loss: 2.721  (Adv: 1.538, Cls: 0.439, Rec: 0.074)\n",
            "Epoch [46/100] Batch 100/333 D_loss: 0.895  G_loss: 3.627  (Adv: 2.638, Cls: 0.147, Rec: 0.084)\n",
            "Epoch [46/100] Batch 150/333 D_loss: 0.867  G_loss: 3.477  (Adv: 2.221, Cls: 0.416, Rec: 0.084)\n",
            "Epoch [46/100] Batch 200/333 D_loss: 1.043  G_loss: 3.080  (Adv: 2.037, Cls: 0.217, Rec: 0.083)\n",
            "Epoch [46/100] Batch 250/333 D_loss: 1.018  G_loss: 4.307  (Adv: 3.033, Cls: 0.494, Rec: 0.078)\n",
            "Epoch [46/100] Batch 300/333 D_loss: 1.196  G_loss: 2.567  (Adv: 1.725, Cls: 0.096, Rec: 0.075)\n",
            "Epoch [47/100] Batch 50/333 D_loss: 1.094  G_loss: 3.849  (Adv: 2.462, Cls: 0.484, Rec: 0.090)\n",
            "Epoch [47/100] Batch 100/333 D_loss: 0.854  G_loss: 2.737  (Adv: 1.525, Cls: 0.279, Rec: 0.093)\n",
            "Epoch [47/100] Batch 150/333 D_loss: 0.890  G_loss: 4.593  (Adv: 3.573, Cls: 0.177, Rec: 0.084)\n",
            "Epoch [47/100] Batch 200/333 D_loss: 0.917  G_loss: 2.435  (Adv: 1.570, Cls: 0.078, Rec: 0.079)\n",
            "Epoch [47/100] Batch 250/333 D_loss: 0.721  G_loss: 3.157  (Adv: 2.225, Cls: 0.136, Rec: 0.080)\n",
            "Epoch [47/100] Batch 300/333 D_loss: 0.771  G_loss: 3.617  (Adv: 2.596, Cls: 0.259, Rec: 0.076)\n",
            "Epoch [48/100] Batch 50/333 D_loss: 1.086  G_loss: 4.226  (Adv: 3.286, Cls: 0.181, Rec: 0.076)\n",
            "Epoch [48/100] Batch 100/333 D_loss: 0.890  G_loss: 3.408  (Adv: 2.529, Cls: 0.079, Rec: 0.080)\n",
            "Epoch [48/100] Batch 150/333 D_loss: 0.980  G_loss: 2.772  (Adv: 1.893, Cls: 0.116, Rec: 0.076)\n",
            "Epoch [48/100] Batch 200/333 D_loss: 1.119  G_loss: 2.821  (Adv: 1.776, Cls: 0.207, Rec: 0.084)\n",
            "Epoch [48/100] Batch 250/333 D_loss: 0.837  G_loss: 3.492  (Adv: 2.136, Cls: 0.616, Rec: 0.074)\n",
            "Epoch [48/100] Batch 300/333 D_loss: 0.855  G_loss: 2.906  (Adv: 2.051, Cls: 0.078, Rec: 0.078)\n",
            "Epoch [49/100] Batch 50/333 D_loss: 0.894  G_loss: 3.018  (Adv: 1.991, Cls: 0.244, Rec: 0.078)\n",
            "Epoch [49/100] Batch 100/333 D_loss: 0.865  G_loss: 3.514  (Adv: 1.709, Cls: 0.938, Rec: 0.087)\n",
            "Epoch [49/100] Batch 150/333 D_loss: 0.690  G_loss: 3.651  (Adv: 2.751, Cls: 0.117, Rec: 0.078)\n",
            "Epoch [49/100] Batch 200/333 D_loss: 0.972  G_loss: 3.497  (Adv: 2.660, Cls: 0.097, Rec: 0.074)\n",
            "Epoch [49/100] Batch 250/333 D_loss: 0.750  G_loss: 2.984  (Adv: 2.077, Cls: 0.129, Rec: 0.078)\n",
            "Epoch [49/100] Batch 300/333 D_loss: 1.333  G_loss: 4.831  (Adv: 3.909, Cls: 0.159, Rec: 0.076)\n",
            "Epoch [50/100] Batch 50/333 D_loss: 1.227  G_loss: 5.322  (Adv: 4.291, Cls: 0.245, Rec: 0.079)\n",
            "Epoch [50/100] Batch 100/333 D_loss: 0.814  G_loss: 3.333  (Adv: 2.346, Cls: 0.119, Rec: 0.087)\n",
            "Epoch [50/100] Batch 150/333 D_loss: 0.797  G_loss: 4.077  (Adv: 2.947, Cls: 0.353, Rec: 0.078)\n",
            "Epoch [50/100] Batch 200/333 D_loss: 0.858  G_loss: 3.262  (Adv: 2.217, Cls: 0.305, Rec: 0.074)\n",
            "Epoch [50/100] Batch 250/333 D_loss: 0.985  G_loss: 3.413  (Adv: 2.345, Cls: 0.309, Rec: 0.076)\n",
            "Epoch [50/100] Batch 300/333 D_loss: 0.734  G_loss: 3.564  (Adv: 2.605, Cls: 0.179, Rec: 0.078)\n",
            "Saved checkpoints at epoch 50\n",
            "Saved sample comparison image at epoch 50\n",
            "Epoch [51/100] Batch 50/333 D_loss: 0.843  G_loss: 3.145  (Adv: 2.128, Cls: 0.250, Rec: 0.077)\n",
            "Epoch [51/100] Batch 100/333 D_loss: 1.121  G_loss: 3.186  (Adv: 2.273, Cls: 0.085, Rec: 0.083)\n",
            "Epoch [51/100] Batch 150/333 D_loss: 1.068  G_loss: 4.049  (Adv: 3.091, Cls: 0.193, Rec: 0.076)\n",
            "Epoch [51/100] Batch 200/333 D_loss: 0.762  G_loss: 3.309  (Adv: 2.174, Cls: 0.364, Rec: 0.077)\n",
            "Epoch [51/100] Batch 250/333 D_loss: 3.757  G_loss: 7.817  (Adv: 5.673, Cls: 1.194, Rec: 0.095)\n",
            "Epoch [51/100] Batch 300/333 D_loss: 0.989  G_loss: 3.075  (Adv: 2.092, Cls: 0.253, Rec: 0.073)\n",
            "Epoch [52/100] Batch 50/333 D_loss: 0.956  G_loss: 2.704  (Adv: 1.830, Cls: 0.132, Rec: 0.074)\n",
            "Epoch [52/100] Batch 100/333 D_loss: 0.892  G_loss: 2.982  (Adv: 1.892, Cls: 0.322, Rec: 0.077)\n",
            "Epoch [52/100] Batch 150/333 D_loss: 1.167  G_loss: 2.546  (Adv: 1.689, Cls: 0.162, Rec: 0.069)\n",
            "Epoch [52/100] Batch 200/333 D_loss: 1.473  G_loss: 4.985  (Adv: 4.039, Cls: 0.104, Rec: 0.084)\n",
            "Epoch [52/100] Batch 250/333 D_loss: 0.650  G_loss: 3.566  (Adv: 2.685, Cls: 0.193, Rec: 0.069)\n",
            "Epoch [52/100] Batch 300/333 D_loss: 1.126  G_loss: 4.527  (Adv: 3.503, Cls: 0.201, Rec: 0.082)\n",
            "Epoch [53/100] Batch 50/333 D_loss: 0.917  G_loss: 3.679  (Adv: 2.563, Cls: 0.352, Rec: 0.076)\n",
            "Epoch [53/100] Batch 100/333 D_loss: 0.849  G_loss: 3.029  (Adv: 2.189, Cls: 0.097, Rec: 0.074)\n",
            "Epoch [53/100] Batch 150/333 D_loss: 1.309  G_loss: 4.070  (Adv: 3.230, Cls: 0.075, Rec: 0.077)\n",
            "Epoch [53/100] Batch 200/333 D_loss: 0.918  G_loss: 3.433  (Adv: 2.535, Cls: 0.198, Rec: 0.070)\n",
            "Epoch [53/100] Batch 250/333 D_loss: 0.792  G_loss: 3.198  (Adv: 2.063, Cls: 0.313, Rec: 0.082)\n",
            "Epoch [53/100] Batch 300/333 D_loss: 0.985  G_loss: 2.433  (Adv: 1.580, Cls: 0.127, Rec: 0.073)\n",
            "Epoch [54/100] Batch 50/333 D_loss: 1.102  G_loss: 2.675  (Adv: 1.860, Cls: 0.090, Rec: 0.073)\n",
            "Epoch [54/100] Batch 100/333 D_loss: 0.790  G_loss: 3.412  (Adv: 2.475, Cls: 0.191, Rec: 0.075)\n",
            "Epoch [54/100] Batch 150/333 D_loss: 0.958  G_loss: 3.017  (Adv: 1.907, Cls: 0.346, Rec: 0.076)\n",
            "Epoch [54/100] Batch 200/333 D_loss: 0.831  G_loss: 3.192  (Adv: 2.285, Cls: 0.176, Rec: 0.073)\n",
            "Epoch [54/100] Batch 250/333 D_loss: 0.651  G_loss: 3.529  (Adv: 2.480, Cls: 0.270, Rec: 0.078)\n",
            "Epoch [54/100] Batch 300/333 D_loss: 0.891  G_loss: 2.937  (Adv: 1.966, Cls: 0.222, Rec: 0.075)\n",
            "Epoch [55/100] Batch 50/333 D_loss: 0.955  G_loss: 2.639  (Adv: 1.794, Cls: 0.118, Rec: 0.073)\n",
            "Epoch [55/100] Batch 100/333 D_loss: 0.962  G_loss: 3.524  (Adv: 2.513, Cls: 0.294, Rec: 0.072)\n",
            "Epoch [55/100] Batch 150/333 D_loss: 0.794  G_loss: 2.855  (Adv: 2.070, Cls: 0.068, Rec: 0.072)\n",
            "Epoch [55/100] Batch 200/333 D_loss: 1.185  G_loss: 4.027  (Adv: 2.960, Cls: 0.238, Rec: 0.083)\n",
            "Epoch [55/100] Batch 250/333 D_loss: 1.379  G_loss: 2.529  (Adv: 1.648, Cls: 0.080, Rec: 0.080)\n",
            "Epoch [55/100] Batch 300/333 D_loss: 0.688  G_loss: 3.427  (Adv: 2.568, Cls: 0.073, Rec: 0.079)\n",
            "Saved checkpoints at epoch 55\n",
            "Saved sample comparison image at epoch 55\n",
            "Epoch [56/100] Batch 50/333 D_loss: 0.823  G_loss: 3.645  (Adv: 2.723, Cls: 0.093, Rec: 0.083)\n",
            "Epoch [56/100] Batch 100/333 D_loss: 0.882  G_loss: 3.169  (Adv: 2.277, Cls: 0.045, Rec: 0.085)\n",
            "Epoch [56/100] Batch 150/333 D_loss: 0.981  G_loss: 3.146  (Adv: 1.655, Cls: 0.690, Rec: 0.080)\n",
            "Epoch [56/100] Batch 200/333 D_loss: 0.786  G_loss: 3.243  (Adv: 2.141, Cls: 0.370, Rec: 0.073)\n",
            "Epoch [56/100] Batch 250/333 D_loss: 1.103  G_loss: 2.495  (Adv: 1.534, Cls: 0.230, Rec: 0.073)\n",
            "Epoch [56/100] Batch 300/333 D_loss: 0.805  G_loss: 3.371  (Adv: 2.477, Cls: 0.155, Rec: 0.074)\n",
            "Epoch [57/100] Batch 50/333 D_loss: 0.791  G_loss: 3.886  (Adv: 2.976, Cls: 0.094, Rec: 0.082)\n",
            "Epoch [57/100] Batch 100/333 D_loss: 1.061  G_loss: 4.131  (Adv: 3.038, Cls: 0.302, Rec: 0.079)\n",
            "Epoch [57/100] Batch 150/333 D_loss: 1.537  G_loss: 2.995  (Adv: 1.554, Cls: 0.690, Rec: 0.075)\n",
            "Epoch [57/100] Batch 200/333 D_loss: 0.801  G_loss: 2.854  (Adv: 1.935, Cls: 0.187, Rec: 0.073)\n",
            "Epoch [57/100] Batch 250/333 D_loss: 1.050  G_loss: 5.374  (Adv: 4.523, Cls: 0.070, Rec: 0.078)\n",
            "Epoch [57/100] Batch 300/333 D_loss: 0.593  G_loss: 3.270  (Adv: 2.417, Cls: 0.104, Rec: 0.075)\n",
            "Epoch [58/100] Batch 50/333 D_loss: 0.857  G_loss: 3.612  (Adv: 2.697, Cls: 0.137, Rec: 0.078)\n",
            "Epoch [58/100] Batch 100/333 D_loss: 1.277  G_loss: 2.847  (Adv: 1.275, Cls: 0.743, Rec: 0.083)\n",
            "Epoch [58/100] Batch 150/333 D_loss: 0.753  G_loss: 4.213  (Adv: 3.250, Cls: 0.205, Rec: 0.076)\n",
            "Epoch [58/100] Batch 200/333 D_loss: 0.994  G_loss: 2.969  (Adv: 1.790, Cls: 0.387, Rec: 0.079)\n",
            "Epoch [58/100] Batch 250/333 D_loss: 1.100  G_loss: 2.565  (Adv: 1.628, Cls: 0.252, Rec: 0.068)\n",
            "Epoch [58/100] Batch 300/333 D_loss: 0.733  G_loss: 3.426  (Adv: 2.416, Cls: 0.229, Rec: 0.078)\n",
            "Epoch [59/100] Batch 50/333 D_loss: 0.786  G_loss: 4.748  (Adv: 3.643, Cls: 0.203, Rec: 0.090)\n",
            "Epoch [59/100] Batch 100/333 D_loss: 0.871  G_loss: 4.644  (Adv: 3.824, Cls: 0.030, Rec: 0.079)\n",
            "Epoch [59/100] Batch 150/333 D_loss: 0.959  G_loss: 2.495  (Adv: 1.645, Cls: 0.161, Rec: 0.069)\n",
            "Epoch [59/100] Batch 200/333 D_loss: 0.869  G_loss: 3.727  (Adv: 2.858, Cls: 0.125, Rec: 0.074)\n",
            "Epoch [59/100] Batch 250/333 D_loss: 0.697  G_loss: 3.799  (Adv: 2.804, Cls: 0.264, Rec: 0.073)\n",
            "Epoch [59/100] Batch 300/333 D_loss: 1.066  G_loss: 3.640  (Adv: 2.426, Cls: 0.385, Rec: 0.083)\n",
            "Epoch [60/100] Batch 50/333 D_loss: 0.818  G_loss: 4.047  (Adv: 3.090, Cls: 0.077, Rec: 0.088)\n",
            "Epoch [60/100] Batch 100/333 D_loss: 0.973  G_loss: 3.343  (Adv: 2.391, Cls: 0.195, Rec: 0.076)\n",
            "Epoch [60/100] Batch 150/333 D_loss: 0.837  G_loss: 3.048  (Adv: 2.025, Cls: 0.316, Rec: 0.071)\n",
            "Epoch [60/100] Batch 200/333 D_loss: 1.045  G_loss: 2.699  (Adv: 1.685, Cls: 0.300, Rec: 0.071)\n",
            "Epoch [60/100] Batch 250/333 D_loss: 0.858  G_loss: 3.980  (Adv: 2.664, Cls: 0.642, Rec: 0.067)\n",
            "Epoch [60/100] Batch 300/333 D_loss: 0.685  G_loss: 3.335  (Adv: 2.131, Cls: 0.424, Rec: 0.078)\n",
            "Saved checkpoints at epoch 60\n",
            "Saved sample comparison image at epoch 60\n",
            "Epoch [61/100] Batch 50/333 D_loss: 0.938  G_loss: 3.889  (Adv: 2.692, Cls: 0.402, Rec: 0.080)\n",
            "Epoch [61/100] Batch 100/333 D_loss: 1.140  G_loss: 3.677  (Adv: 2.919, Cls: 0.057, Rec: 0.070)\n",
            "Epoch [61/100] Batch 150/333 D_loss: 0.892  G_loss: 3.120  (Adv: 2.182, Cls: 0.213, Rec: 0.072)\n",
            "Epoch [61/100] Batch 200/333 D_loss: 0.814  G_loss: 3.311  (Adv: 2.312, Cls: 0.234, Rec: 0.076)\n",
            "Epoch [61/100] Batch 250/333 D_loss: 1.112  G_loss: 3.432  (Adv: 1.909, Cls: 0.737, Rec: 0.079)\n",
            "Epoch [61/100] Batch 300/333 D_loss: 0.951  G_loss: 4.074  (Adv: 2.870, Cls: 0.482, Rec: 0.072)\n",
            "Epoch [62/100] Batch 50/333 D_loss: 0.843  G_loss: 4.181  (Adv: 3.366, Cls: 0.092, Rec: 0.072)\n",
            "Epoch [62/100] Batch 100/333 D_loss: 0.741  G_loss: 2.721  (Adv: 1.889, Cls: 0.139, Rec: 0.069)\n",
            "Epoch [62/100] Batch 150/333 D_loss: 0.754  G_loss: 3.226  (Adv: 1.986, Cls: 0.492, Rec: 0.075)\n",
            "Epoch [62/100] Batch 200/333 D_loss: 1.279  G_loss: 3.339  (Adv: 2.252, Cls: 0.340, Rec: 0.075)\n",
            "Epoch [62/100] Batch 250/333 D_loss: 1.330  G_loss: 3.698  (Adv: 2.872, Cls: 0.084, Rec: 0.074)\n",
            "Epoch [62/100] Batch 300/333 D_loss: 0.916  G_loss: 3.941  (Adv: 2.766, Cls: 0.406, Rec: 0.077)\n",
            "Epoch [63/100] Batch 50/333 D_loss: 0.829  G_loss: 3.381  (Adv: 2.241, Cls: 0.379, Rec: 0.076)\n",
            "Epoch [63/100] Batch 100/333 D_loss: 1.170  G_loss: 2.932  (Adv: 1.654, Cls: 0.484, Rec: 0.079)\n",
            "Epoch [63/100] Batch 150/333 D_loss: 1.122  G_loss: 3.183  (Adv: 1.930, Cls: 0.454, Rec: 0.080)\n",
            "Epoch [63/100] Batch 200/333 D_loss: 1.297  G_loss: 4.158  (Adv: 3.353, Cls: 0.084, Rec: 0.072)\n",
            "Epoch [63/100] Batch 250/333 D_loss: 0.950  G_loss: 2.653  (Adv: 1.859, Cls: 0.037, Rec: 0.076)\n",
            "Epoch [63/100] Batch 300/333 D_loss: 0.833  G_loss: 4.532  (Adv: 3.607, Cls: 0.106, Rec: 0.082)\n",
            "Epoch [64/100] Batch 50/333 D_loss: 0.751  G_loss: 4.136  (Adv: 3.278, Cls: 0.069, Rec: 0.079)\n",
            "Epoch [64/100] Batch 100/333 D_loss: 0.966  G_loss: 3.829  (Adv: 2.962, Cls: 0.106, Rec: 0.076)\n",
            "Epoch [64/100] Batch 150/333 D_loss: 0.662  G_loss: 4.436  (Adv: 3.505, Cls: 0.193, Rec: 0.074)\n",
            "Epoch [64/100] Batch 200/333 D_loss: 0.789  G_loss: 3.022  (Adv: 1.827, Cls: 0.421, Rec: 0.077)\n",
            "Epoch [64/100] Batch 250/333 D_loss: 0.739  G_loss: 3.024  (Adv: 2.164, Cls: 0.135, Rec: 0.072)\n",
            "Epoch [64/100] Batch 300/333 D_loss: 0.936  G_loss: 4.499  (Adv: 3.414, Cls: 0.324, Rec: 0.076)\n",
            "Epoch [65/100] Batch 50/333 D_loss: 1.819  G_loss: 2.415  (Adv: 1.250, Cls: 0.404, Rec: 0.076)\n",
            "Epoch [65/100] Batch 100/333 D_loss: 0.776  G_loss: 3.001  (Adv: 2.090, Cls: 0.198, Rec: 0.071)\n",
            "Epoch [65/100] Batch 150/333 D_loss: 1.074  G_loss: 3.596  (Adv: 2.322, Cls: 0.496, Rec: 0.078)\n",
            "Epoch [65/100] Batch 200/333 D_loss: 0.860  G_loss: 3.270  (Adv: 2.115, Cls: 0.429, Rec: 0.073)\n",
            "Epoch [65/100] Batch 250/333 D_loss: 0.874  G_loss: 3.074  (Adv: 2.104, Cls: 0.190, Rec: 0.078)\n",
            "Epoch [65/100] Batch 300/333 D_loss: 0.817  G_loss: 4.050  (Adv: 3.169, Cls: 0.155, Rec: 0.073)\n",
            "Saved checkpoints at epoch 65\n",
            "Saved sample comparison image at epoch 65\n",
            "Epoch [66/100] Batch 50/333 D_loss: 1.112  G_loss: 2.598  (Adv: 1.696, Cls: 0.139, Rec: 0.076)\n",
            "Epoch [66/100] Batch 100/333 D_loss: 0.842  G_loss: 2.585  (Adv: 1.473, Cls: 0.362, Rec: 0.075)\n",
            "Epoch [66/100] Batch 150/333 D_loss: 0.812  G_loss: 3.170  (Adv: 2.071, Cls: 0.340, Rec: 0.076)\n",
            "Epoch [66/100] Batch 200/333 D_loss: 1.035  G_loss: 4.232  (Adv: 1.942, Cls: 1.532, Rec: 0.076)\n",
            "Epoch [66/100] Batch 250/333 D_loss: 0.976  G_loss: 4.796  (Adv: 3.712, Cls: 0.309, Rec: 0.077)\n",
            "Epoch [66/100] Batch 300/333 D_loss: 1.224  G_loss: 5.275  (Adv: 3.947, Cls: 0.583, Rec: 0.075)\n",
            "Epoch [67/100] Batch 50/333 D_loss: 0.750  G_loss: 4.183  (Adv: 3.311, Cls: 0.101, Rec: 0.077)\n",
            "Epoch [67/100] Batch 100/333 D_loss: 0.973  G_loss: 3.412  (Adv: 2.641, Cls: 0.052, Rec: 0.072)\n",
            "Epoch [67/100] Batch 150/333 D_loss: 0.943  G_loss: 3.075  (Adv: 2.182, Cls: 0.154, Rec: 0.074)\n",
            "Epoch [67/100] Batch 200/333 D_loss: 0.631  G_loss: 5.020  (Adv: 2.615, Cls: 1.617, Rec: 0.079)\n",
            "Epoch [67/100] Batch 250/333 D_loss: 0.815  G_loss: 2.755  (Adv: 1.855, Cls: 0.100, Rec: 0.080)\n",
            "Epoch [67/100] Batch 300/333 D_loss: 0.867  G_loss: 3.768  (Adv: 1.908, Cls: 1.090, Rec: 0.077)\n",
            "Epoch [68/100] Batch 50/333 D_loss: 0.906  G_loss: 3.292  (Adv: 1.938, Cls: 0.679, Rec: 0.067)\n",
            "Epoch [68/100] Batch 100/333 D_loss: 0.838  G_loss: 3.999  (Adv: 2.949, Cls: 0.221, Rec: 0.083)\n",
            "Epoch [68/100] Batch 150/333 D_loss: 0.895  G_loss: 3.551  (Adv: 2.363, Cls: 0.402, Rec: 0.079)\n",
            "Epoch [68/100] Batch 200/333 D_loss: 0.637  G_loss: 2.945  (Adv: 2.012, Cls: 0.201, Rec: 0.073)\n",
            "Epoch [68/100] Batch 250/333 D_loss: 0.911  G_loss: 4.035  (Adv: 3.177, Cls: 0.141, Rec: 0.072)\n",
            "Epoch [68/100] Batch 300/333 D_loss: 1.015  G_loss: 5.350  (Adv: 4.308, Cls: 0.215, Rec: 0.083)\n",
            "Epoch [69/100] Batch 50/333 D_loss: 0.695  G_loss: 3.216  (Adv: 2.464, Cls: 0.031, Rec: 0.072)\n",
            "Epoch [69/100] Batch 100/333 D_loss: 0.762  G_loss: 5.023  (Adv: 3.883, Cls: 0.297, Rec: 0.084)\n",
            "Epoch [69/100] Batch 150/333 D_loss: 0.997  G_loss: 3.515  (Adv: 2.138, Cls: 0.671, Rec: 0.071)\n",
            "Epoch [69/100] Batch 200/333 D_loss: 0.932  G_loss: 3.402  (Adv: 2.516, Cls: 0.185, Rec: 0.070)\n",
            "Epoch [69/100] Batch 250/333 D_loss: 0.827  G_loss: 3.816  (Adv: 2.543, Cls: 0.414, Rec: 0.086)\n",
            "Epoch [69/100] Batch 300/333 D_loss: 0.727  G_loss: 4.440  (Adv: 3.411, Cls: 0.165, Rec: 0.086)\n",
            "Epoch [70/100] Batch 50/333 D_loss: 0.670  G_loss: 2.477  (Adv: 1.424, Cls: 0.310, Rec: 0.074)\n",
            "Epoch [70/100] Batch 100/333 D_loss: 0.643  G_loss: 3.294  (Adv: 2.277, Cls: 0.292, Rec: 0.073)\n",
            "Epoch [70/100] Batch 150/333 D_loss: 1.458  G_loss: 2.884  (Adv: 1.622, Cls: 0.561, Rec: 0.070)\n",
            "Epoch [70/100] Batch 200/333 D_loss: 1.002  G_loss: 4.373  (Adv: 3.514, Cls: 0.069, Rec: 0.079)\n",
            "Epoch [70/100] Batch 250/333 D_loss: 0.856  G_loss: 3.600  (Adv: 1.757, Cls: 1.132, Rec: 0.071)\n",
            "Epoch [70/100] Batch 300/333 D_loss: 0.733  G_loss: 3.136  (Adv: 2.224, Cls: 0.251, Rec: 0.066)\n",
            "Saved checkpoints at epoch 70\n",
            "Saved sample comparison image at epoch 70\n",
            "Epoch [71/100] Batch 50/333 D_loss: 0.498  G_loss: 3.811  (Adv: 2.970, Cls: 0.072, Rec: 0.077)\n",
            "Epoch [71/100] Batch 100/333 D_loss: 0.687  G_loss: 3.302  (Adv: 2.472, Cls: 0.118, Rec: 0.071)\n",
            "Epoch [71/100] Batch 150/333 D_loss: 1.176  G_loss: 4.276  (Adv: 3.403, Cls: 0.159, Rec: 0.071)\n",
            "Epoch [71/100] Batch 200/333 D_loss: 0.852  G_loss: 3.386  (Adv: 2.254, Cls: 0.419, Rec: 0.071)\n",
            "Epoch [71/100] Batch 250/333 D_loss: 1.152  G_loss: 3.495  (Adv: 2.323, Cls: 0.416, Rec: 0.076)\n",
            "Epoch [71/100] Batch 300/333 D_loss: 0.875  G_loss: 3.234  (Adv: 2.346, Cls: 0.195, Rec: 0.069)\n",
            "Epoch [72/100] Batch 50/333 D_loss: 0.894  G_loss: 2.971  (Adv: 2.032, Cls: 0.245, Rec: 0.069)\n",
            "Epoch [72/100] Batch 100/333 D_loss: 0.969  G_loss: 3.407  (Adv: 2.577, Cls: 0.041, Rec: 0.079)\n",
            "Epoch [72/100] Batch 150/333 D_loss: 0.813  G_loss: 3.534  (Adv: 2.403, Cls: 0.485, Rec: 0.065)\n",
            "Epoch [72/100] Batch 200/333 D_loss: 0.750  G_loss: 3.024  (Adv: 1.766, Cls: 0.534, Rec: 0.072)\n",
            "Epoch [72/100] Batch 250/333 D_loss: 1.033  G_loss: 3.880  (Adv: 2.966, Cls: 0.118, Rec: 0.080)\n",
            "Epoch [72/100] Batch 300/333 D_loss: 0.931  G_loss: 3.517  (Adv: 2.402, Cls: 0.208, Rec: 0.091)\n",
            "Epoch [73/100] Batch 50/333 D_loss: 1.127  G_loss: 2.153  (Adv: 1.200, Cls: 0.261, Rec: 0.069)\n",
            "Epoch [73/100] Batch 100/333 D_loss: 0.798  G_loss: 4.293  (Adv: 2.980, Cls: 0.455, Rec: 0.086)\n",
            "Epoch [73/100] Batch 150/333 D_loss: 1.053  G_loss: 2.702  (Adv: 1.771, Cls: 0.138, Rec: 0.079)\n",
            "Epoch [73/100] Batch 200/333 D_loss: 1.138  G_loss: 2.637  (Adv: 1.574, Cls: 0.326, Rec: 0.074)\n",
            "Epoch [73/100] Batch 250/333 D_loss: 1.045  G_loss: 3.227  (Adv: 2.123, Cls: 0.384, Rec: 0.072)\n",
            "Epoch [73/100] Batch 300/333 D_loss: 0.801  G_loss: 3.157  (Adv: 2.292, Cls: 0.051, Rec: 0.082)\n",
            "Epoch [74/100] Batch 50/333 D_loss: 1.358  G_loss: 5.291  (Adv: 4.187, Cls: 0.383, Rec: 0.072)\n",
            "Epoch [74/100] Batch 100/333 D_loss: 0.690  G_loss: 3.105  (Adv: 2.345, Cls: 0.064, Rec: 0.070)\n",
            "Epoch [74/100] Batch 150/333 D_loss: 0.994  G_loss: 2.952  (Adv: 2.034, Cls: 0.088, Rec: 0.083)\n",
            "Epoch [74/100] Batch 200/333 D_loss: 0.677  G_loss: 4.061  (Adv: 2.967, Cls: 0.225, Rec: 0.087)\n",
            "Epoch [74/100] Batch 250/333 D_loss: 0.800  G_loss: 2.885  (Adv: 1.753, Cls: 0.341, Rec: 0.079)\n",
            "Epoch [74/100] Batch 300/333 D_loss: 0.926  G_loss: 3.415  (Adv: 2.130, Cls: 0.563, Rec: 0.072)\n",
            "Epoch [75/100] Batch 50/333 D_loss: 0.760  G_loss: 3.181  (Adv: 2.141, Cls: 0.276, Rec: 0.076)\n",
            "Epoch [75/100] Batch 100/333 D_loss: 0.724  G_loss: 3.272  (Adv: 2.353, Cls: 0.227, Rec: 0.069)\n",
            "Epoch [75/100] Batch 150/333 D_loss: 0.721  G_loss: 6.297  (Adv: 5.193, Cls: 0.344, Rec: 0.076)\n",
            "Epoch [75/100] Batch 200/333 D_loss: 0.649  G_loss: 3.228  (Adv: 2.277, Cls: 0.212, Rec: 0.074)\n",
            "Epoch [75/100] Batch 250/333 D_loss: 0.820  G_loss: 3.436  (Adv: 2.304, Cls: 0.351, Rec: 0.078)\n",
            "Epoch [75/100] Batch 300/333 D_loss: 0.804  G_loss: 3.968  (Adv: 2.961, Cls: 0.320, Rec: 0.069)\n",
            "Saved checkpoints at epoch 75\n",
            "Saved sample comparison image at epoch 75\n",
            "Epoch [76/100] Batch 50/333 D_loss: 0.878  G_loss: 3.176  (Adv: 2.344, Cls: 0.100, Rec: 0.073)\n",
            "Epoch [76/100] Batch 100/333 D_loss: 1.250  G_loss: 2.773  (Adv: 1.505, Cls: 0.528, Rec: 0.074)\n",
            "Epoch [76/100] Batch 150/333 D_loss: 0.894  G_loss: 4.066  (Adv: 3.012, Cls: 0.391, Rec: 0.066)\n",
            "Epoch [76/100] Batch 200/333 D_loss: 0.696  G_loss: 3.708  (Adv: 2.831, Cls: 0.168, Rec: 0.071)\n",
            "Epoch [76/100] Batch 250/333 D_loss: 0.864  G_loss: 4.860  (Adv: 3.838, Cls: 0.233, Rec: 0.079)\n",
            "Epoch [76/100] Batch 300/333 D_loss: 0.757  G_loss: 3.597  (Adv: 2.502, Cls: 0.398, Rec: 0.070)\n",
            "Epoch [77/100] Batch 50/333 D_loss: 0.834  G_loss: 4.503  (Adv: 3.347, Cls: 0.372, Rec: 0.078)\n",
            "Epoch [77/100] Batch 100/333 D_loss: 0.729  G_loss: 4.008  (Adv: 2.854, Cls: 0.425, Rec: 0.073)\n",
            "Epoch [77/100] Batch 150/333 D_loss: 0.731  G_loss: 3.497  (Adv: 2.520, Cls: 0.167, Rec: 0.081)\n",
            "Epoch [77/100] Batch 200/333 D_loss: 0.816  G_loss: 4.245  (Adv: 3.260, Cls: 0.222, Rec: 0.076)\n",
            "Epoch [77/100] Batch 250/333 D_loss: 0.798  G_loss: 4.957  (Adv: 3.497, Cls: 0.720, Rec: 0.074)\n",
            "Epoch [77/100] Batch 300/333 D_loss: 0.837  G_loss: 3.041  (Adv: 2.270, Cls: 0.066, Rec: 0.070)\n",
            "Epoch [78/100] Batch 50/333 D_loss: 0.877  G_loss: 3.696  (Adv: 2.818, Cls: 0.106, Rec: 0.077)\n",
            "Epoch [78/100] Batch 100/333 D_loss: 0.848  G_loss: 4.194  (Adv: 3.155, Cls: 0.202, Rec: 0.084)\n",
            "Epoch [78/100] Batch 150/333 D_loss: 0.954  G_loss: 5.277  (Adv: 4.242, Cls: 0.279, Rec: 0.076)\n",
            "Epoch [78/100] Batch 200/333 D_loss: 0.670  G_loss: 4.344  (Adv: 3.325, Cls: 0.167, Rec: 0.085)\n",
            "Epoch [78/100] Batch 250/333 D_loss: 0.692  G_loss: 3.802  (Adv: 2.867, Cls: 0.218, Rec: 0.072)\n",
            "Epoch [78/100] Batch 300/333 D_loss: 0.863  G_loss: 3.231  (Adv: 2.322, Cls: 0.193, Rec: 0.072)\n",
            "Epoch [79/100] Batch 50/333 D_loss: 0.729  G_loss: 4.924  (Adv: 3.914, Cls: 0.174, Rec: 0.084)\n",
            "Epoch [79/100] Batch 100/333 D_loss: 0.591  G_loss: 3.707  (Adv: 2.904, Cls: 0.077, Rec: 0.073)\n",
            "Epoch [79/100] Batch 150/333 D_loss: 1.290  G_loss: 5.364  (Adv: 4.518, Cls: 0.158, Rec: 0.069)\n",
            "Epoch [79/100] Batch 200/333 D_loss: 1.351  G_loss: 2.903  (Adv: 1.280, Cls: 0.850, Rec: 0.077)\n",
            "Epoch [79/100] Batch 250/333 D_loss: 1.214  G_loss: 7.088  (Adv: 5.923, Cls: 0.276, Rec: 0.089)\n",
            "Epoch [79/100] Batch 300/333 D_loss: 1.313  G_loss: 2.311  (Adv: 1.330, Cls: 0.290, Rec: 0.069)\n",
            "Epoch [80/100] Batch 50/333 D_loss: 0.928  G_loss: 3.394  (Adv: 2.449, Cls: 0.202, Rec: 0.074)\n",
            "Epoch [80/100] Batch 100/333 D_loss: 0.680  G_loss: 2.889  (Adv: 1.832, Cls: 0.354, Rec: 0.070)\n",
            "Epoch [80/100] Batch 150/333 D_loss: 0.725  G_loss: 4.611  (Adv: 3.591, Cls: 0.265, Rec: 0.076)\n",
            "Epoch [80/100] Batch 200/333 D_loss: 1.610  G_loss: 4.608  (Adv: 3.236, Cls: 0.587, Rec: 0.079)\n",
            "Epoch [80/100] Batch 250/333 D_loss: 0.884  G_loss: 3.532  (Adv: 2.600, Cls: 0.234, Rec: 0.070)\n",
            "Epoch [80/100] Batch 300/333 D_loss: 0.714  G_loss: 3.455  (Adv: 2.232, Cls: 0.465, Rec: 0.076)\n",
            "Saved checkpoints at epoch 80\n",
            "Saved sample comparison image at epoch 80\n",
            "Epoch [81/100] Batch 50/333 D_loss: 0.922  G_loss: 2.945  (Adv: 1.529, Cls: 0.586, Rec: 0.083)\n",
            "Epoch [81/100] Batch 100/333 D_loss: 0.804  G_loss: 4.537  (Adv: 3.636, Cls: 0.208, Rec: 0.069)\n",
            "Epoch [81/100] Batch 150/333 D_loss: 0.678  G_loss: 4.143  (Adv: 3.351, Cls: 0.039, Rec: 0.075)\n",
            "Epoch [81/100] Batch 200/333 D_loss: 0.725  G_loss: 5.166  (Adv: 4.350, Cls: 0.017, Rec: 0.080)\n",
            "Epoch [81/100] Batch 250/333 D_loss: 0.807  G_loss: 6.457  (Adv: 4.542, Cls: 1.131, Rec: 0.078)\n",
            "Epoch [81/100] Batch 300/333 D_loss: 0.670  G_loss: 3.754  (Adv: 2.859, Cls: 0.130, Rec: 0.076)\n",
            "Epoch [82/100] Batch 50/333 D_loss: 0.551  G_loss: 3.263  (Adv: 2.271, Cls: 0.139, Rec: 0.085)\n",
            "Epoch [82/100] Batch 100/333 D_loss: 0.802  G_loss: 3.198  (Adv: 2.115, Cls: 0.366, Rec: 0.072)\n",
            "Epoch [82/100] Batch 150/333 D_loss: 1.019  G_loss: 4.151  (Adv: 3.284, Cls: 0.134, Rec: 0.073)\n",
            "Epoch [82/100] Batch 200/333 D_loss: 0.747  G_loss: 4.331  (Adv: 3.465, Cls: 0.085, Rec: 0.078)\n",
            "Epoch [82/100] Batch 250/333 D_loss: 0.815  G_loss: 4.130  (Adv: 3.079, Cls: 0.294, Rec: 0.076)\n",
            "Epoch [82/100] Batch 300/333 D_loss: 0.719  G_loss: 3.838  (Adv: 2.954, Cls: 0.157, Rec: 0.073)\n",
            "Epoch [83/100] Batch 50/333 D_loss: 0.712  G_loss: 4.191  (Adv: 2.964, Cls: 0.470, Rec: 0.076)\n",
            "Epoch [83/100] Batch 100/333 D_loss: 0.586  G_loss: 3.378  (Adv: 2.506, Cls: 0.115, Rec: 0.076)\n",
            "Epoch [83/100] Batch 150/333 D_loss: 1.041  G_loss: 2.716  (Adv: 1.842, Cls: 0.166, Rec: 0.071)\n",
            "Epoch [83/100] Batch 200/333 D_loss: 0.729  G_loss: 4.973  (Adv: 4.086, Cls: 0.166, Rec: 0.072)\n",
            "Epoch [83/100] Batch 250/333 D_loss: 0.844  G_loss: 4.879  (Adv: 3.694, Cls: 0.478, Rec: 0.071)\n",
            "Epoch [83/100] Batch 300/333 D_loss: 0.706  G_loss: 4.032  (Adv: 2.887, Cls: 0.253, Rec: 0.089)\n",
            "Epoch [84/100] Batch 50/333 D_loss: 0.833  G_loss: 3.166  (Adv: 1.814, Cls: 0.655, Rec: 0.070)\n",
            "Epoch [84/100] Batch 100/333 D_loss: 0.611  G_loss: 3.177  (Adv: 2.158, Cls: 0.100, Rec: 0.092)\n",
            "Epoch [84/100] Batch 150/333 D_loss: 0.668  G_loss: 5.031  (Adv: 4.112, Cls: 0.223, Rec: 0.070)\n",
            "Epoch [84/100] Batch 200/333 D_loss: 1.005  G_loss: 3.005  (Adv: 1.905, Cls: 0.372, Rec: 0.073)\n",
            "Epoch [84/100] Batch 250/333 D_loss: 0.745  G_loss: 3.630  (Adv: 2.565, Cls: 0.376, Rec: 0.069)\n",
            "Epoch [84/100] Batch 300/333 D_loss: 0.723  G_loss: 3.514  (Adv: 2.412, Cls: 0.390, Rec: 0.071)\n",
            "Epoch [85/100] Batch 50/333 D_loss: 0.674  G_loss: 3.372  (Adv: 2.390, Cls: 0.222, Rec: 0.076)\n",
            "Epoch [85/100] Batch 100/333 D_loss: 0.870  G_loss: 5.401  (Adv: 4.512, Cls: 0.115, Rec: 0.077)\n",
            "Epoch [85/100] Batch 150/333 D_loss: 0.625  G_loss: 5.027  (Adv: 4.029, Cls: 0.136, Rec: 0.086)\n",
            "Epoch [85/100] Batch 200/333 D_loss: 0.636  G_loss: 3.399  (Adv: 2.032, Cls: 0.511, Rec: 0.086)\n",
            "Epoch [85/100] Batch 250/333 D_loss: 1.071  G_loss: 2.876  (Adv: 1.648, Cls: 0.442, Rec: 0.079)\n",
            "Epoch [85/100] Batch 300/333 D_loss: 0.586  G_loss: 2.783  (Adv: 1.937, Cls: 0.102, Rec: 0.074)\n",
            "Saved checkpoints at epoch 85\n",
            "Saved sample comparison image at epoch 85\n",
            "Epoch [86/100] Batch 50/333 D_loss: 1.067  G_loss: 3.578  (Adv: 2.373, Cls: 0.534, Rec: 0.067)\n",
            "Epoch [86/100] Batch 100/333 D_loss: 0.533  G_loss: 3.859  (Adv: 2.756, Cls: 0.315, Rec: 0.079)\n",
            "Epoch [86/100] Batch 150/333 D_loss: 0.702  G_loss: 3.375  (Adv: 2.310, Cls: 0.358, Rec: 0.071)\n",
            "Epoch [86/100] Batch 200/333 D_loss: 1.516  G_loss: 2.778  (Adv: 1.203, Cls: 0.807, Rec: 0.077)\n",
            "Epoch [86/100] Batch 250/333 D_loss: 0.747  G_loss: 4.545  (Adv: 3.564, Cls: 0.270, Rec: 0.071)\n",
            "Epoch [86/100] Batch 300/333 D_loss: 0.671  G_loss: 4.852  (Adv: 3.821, Cls: 0.235, Rec: 0.080)\n",
            "Epoch [87/100] Batch 50/333 D_loss: 0.663  G_loss: 3.637  (Adv: 2.280, Cls: 0.427, Rec: 0.093)\n",
            "Epoch [87/100] Batch 100/333 D_loss: 0.972  G_loss: 4.962  (Adv: 3.955, Cls: 0.360, Rec: 0.065)\n",
            "Epoch [87/100] Batch 150/333 D_loss: 0.700  G_loss: 2.937  (Adv: 2.102, Cls: 0.167, Rec: 0.067)\n",
            "Epoch [87/100] Batch 200/333 D_loss: 0.862  G_loss: 3.436  (Adv: 2.488, Cls: 0.228, Rec: 0.072)\n",
            "Epoch [87/100] Batch 250/333 D_loss: 1.564  G_loss: 4.157  (Adv: 2.940, Cls: 0.459, Rec: 0.076)\n",
            "Epoch [87/100] Batch 300/333 D_loss: 1.159  G_loss: 3.210  (Adv: 2.259, Cls: 0.296, Rec: 0.066)\n",
            "Epoch [88/100] Batch 50/333 D_loss: 0.819  G_loss: 3.545  (Adv: 2.612, Cls: 0.195, Rec: 0.074)\n",
            "Epoch [88/100] Batch 100/333 D_loss: 0.757  G_loss: 3.599  (Adv: 2.807, Cls: 0.112, Rec: 0.068)\n",
            "Epoch [88/100] Batch 150/333 D_loss: 1.451  G_loss: 2.817  (Adv: 1.403, Cls: 0.618, Rec: 0.080)\n",
            "Epoch [88/100] Batch 200/333 D_loss: 0.908  G_loss: 2.782  (Adv: 1.674, Cls: 0.405, Rec: 0.070)\n",
            "Epoch [88/100] Batch 250/333 D_loss: 0.828  G_loss: 3.828  (Adv: 2.990, Cls: 0.140, Rec: 0.070)\n",
            "Epoch [88/100] Batch 300/333 D_loss: 1.306  G_loss: 2.752  (Adv: 1.832, Cls: 0.206, Rec: 0.071)\n",
            "Epoch [89/100] Batch 50/333 D_loss: 0.892  G_loss: 2.766  (Adv: 1.695, Cls: 0.300, Rec: 0.077)\n",
            "Epoch [89/100] Batch 100/333 D_loss: 0.687  G_loss: 4.214  (Adv: 3.297, Cls: 0.186, Rec: 0.073)\n",
            "Epoch [89/100] Batch 150/333 D_loss: 0.778  G_loss: 4.985  (Adv: 4.087, Cls: 0.151, Rec: 0.075)\n",
            "Epoch [89/100] Batch 200/333 D_loss: 0.581  G_loss: 4.453  (Adv: 3.431, Cls: 0.169, Rec: 0.085)\n",
            "Epoch [89/100] Batch 250/333 D_loss: 0.888  G_loss: 3.295  (Adv: 2.307, Cls: 0.283, Rec: 0.071)\n",
            "Epoch [89/100] Batch 300/333 D_loss: 0.795  G_loss: 3.809  (Adv: 2.976, Cls: 0.138, Rec: 0.069)\n",
            "Epoch [90/100] Batch 50/333 D_loss: 0.714  G_loss: 4.461  (Adv: 3.683, Cls: 0.026, Rec: 0.075)\n",
            "Epoch [90/100] Batch 100/333 D_loss: 1.560  G_loss: 2.139  (Adv: 1.205, Cls: 0.227, Rec: 0.071)\n",
            "Epoch [90/100] Batch 150/333 D_loss: 0.582  G_loss: 3.475  (Adv: 2.358, Cls: 0.441, Rec: 0.068)\n",
            "Epoch [90/100] Batch 200/333 D_loss: 0.510  G_loss: 3.624  (Adv: 2.637, Cls: 0.306, Rec: 0.068)\n",
            "Epoch [90/100] Batch 250/333 D_loss: 0.688  G_loss: 5.042  (Adv: 4.146, Cls: 0.138, Rec: 0.076)\n",
            "Epoch [90/100] Batch 300/333 D_loss: 1.072  G_loss: 3.190  (Adv: 2.015, Cls: 0.373, Rec: 0.080)\n",
            "Saved checkpoints at epoch 90\n",
            "Saved sample comparison image at epoch 90\n",
            "Epoch [91/100] Batch 50/333 D_loss: 0.789  G_loss: 3.759  (Adv: 2.688, Cls: 0.368, Rec: 0.070)\n",
            "Epoch [91/100] Batch 100/333 D_loss: 0.575  G_loss: 4.769  (Adv: 3.929, Cls: 0.056, Rec: 0.078)\n",
            "Epoch [91/100] Batch 150/333 D_loss: 0.637  G_loss: 3.797  (Adv: 2.967, Cls: 0.082, Rec: 0.075)\n",
            "Epoch [91/100] Batch 200/333 D_loss: 0.573  G_loss: 3.394  (Adv: 2.470, Cls: 0.209, Rec: 0.071)\n",
            "Epoch [91/100] Batch 250/333 D_loss: 1.198  G_loss: 3.024  (Adv: 1.373, Cls: 0.907, Rec: 0.074)\n",
            "Epoch [91/100] Batch 300/333 D_loss: 1.013  G_loss: 3.544  (Adv: 2.628, Cls: 0.094, Rec: 0.082)\n",
            "Epoch [92/100] Batch 50/333 D_loss: 0.607  G_loss: 4.449  (Adv: 3.364, Cls: 0.192, Rec: 0.089)\n",
            "Epoch [92/100] Batch 100/333 D_loss: 0.728  G_loss: 3.276  (Adv: 1.716, Cls: 0.797, Rec: 0.076)\n",
            "Epoch [92/100] Batch 150/333 D_loss: 0.487  G_loss: 5.718  (Adv: 4.676, Cls: 0.228, Rec: 0.081)\n",
            "Epoch [92/100] Batch 200/333 D_loss: 0.857  G_loss: 3.641  (Adv: 2.762, Cls: 0.138, Rec: 0.074)\n",
            "Epoch [92/100] Batch 250/333 D_loss: 0.705  G_loss: 3.891  (Adv: 3.138, Cls: 0.069, Rec: 0.068)\n",
            "Epoch [92/100] Batch 300/333 D_loss: 0.675  G_loss: 5.322  (Adv: 4.124, Cls: 0.407, Rec: 0.079)\n",
            "Epoch [93/100] Batch 50/333 D_loss: 0.932  G_loss: 3.556  (Adv: 2.456, Cls: 0.427, Rec: 0.067)\n",
            "Epoch [93/100] Batch 100/333 D_loss: 0.580  G_loss: 3.218  (Adv: 2.176, Cls: 0.326, Rec: 0.072)\n",
            "Epoch [93/100] Batch 150/333 D_loss: 0.663  G_loss: 3.816  (Adv: 2.428, Cls: 0.589, Rec: 0.080)\n",
            "Epoch [93/100] Batch 200/333 D_loss: 0.660  G_loss: 5.189  (Adv: 3.380, Cls: 0.937, Rec: 0.087)\n",
            "Epoch [93/100] Batch 250/333 D_loss: 0.666  G_loss: 4.525  (Adv: 3.705, Cls: 0.065, Rec: 0.075)\n",
            "Epoch [93/100] Batch 300/333 D_loss: 0.526  G_loss: 4.219  (Adv: 3.214, Cls: 0.244, Rec: 0.076)\n",
            "Epoch [94/100] Batch 50/333 D_loss: 0.458  G_loss: 4.169  (Adv: 2.391, Cls: 0.873, Rec: 0.091)\n",
            "Epoch [94/100] Batch 100/333 D_loss: 0.595  G_loss: 3.096  (Adv: 2.228, Cls: 0.230, Rec: 0.064)\n",
            "Epoch [94/100] Batch 150/333 D_loss: 1.027  G_loss: 3.406  (Adv: 2.319, Cls: 0.324, Rec: 0.076)\n",
            "Epoch [94/100] Batch 200/333 D_loss: 0.957  G_loss: 3.265  (Adv: 2.343, Cls: 0.236, Rec: 0.069)\n",
            "Epoch [94/100] Batch 250/333 D_loss: 0.673  G_loss: 3.601  (Adv: 2.667, Cls: 0.280, Rec: 0.065)\n",
            "Epoch [94/100] Batch 300/333 D_loss: 0.648  G_loss: 4.277  (Adv: 3.442, Cls: 0.127, Rec: 0.071)\n",
            "Epoch [95/100] Batch 50/333 D_loss: 1.020  G_loss: 3.065  (Adv: 1.650, Cls: 0.668, Rec: 0.075)\n",
            "Epoch [95/100] Batch 100/333 D_loss: 0.636  G_loss: 3.126  (Adv: 2.137, Cls: 0.268, Rec: 0.072)\n",
            "Epoch [95/100] Batch 150/333 D_loss: 0.733  G_loss: 5.006  (Adv: 3.915, Cls: 0.319, Rec: 0.077)\n",
            "Epoch [95/100] Batch 200/333 D_loss: 0.714  G_loss: 3.773  (Adv: 2.730, Cls: 0.248, Rec: 0.080)\n",
            "Epoch [95/100] Batch 250/333 D_loss: 0.581  G_loss: 5.957  (Adv: 3.946, Cls: 1.277, Rec: 0.074)\n",
            "Epoch [95/100] Batch 300/333 D_loss: 0.735  G_loss: 4.933  (Adv: 3.939, Cls: 0.274, Rec: 0.072)\n",
            "Saved checkpoints at epoch 95\n",
            "Saved sample comparison image at epoch 95\n",
            "Epoch [96/100] Batch 50/333 D_loss: 0.830  G_loss: 5.263  (Adv: 4.204, Cls: 0.354, Rec: 0.071)\n",
            "Epoch [96/100] Batch 100/333 D_loss: 0.627  G_loss: 4.048  (Adv: 2.874, Cls: 0.288, Rec: 0.089)\n",
            "Epoch [96/100] Batch 150/333 D_loss: 0.960  G_loss: 2.489  (Adv: 1.295, Cls: 0.398, Rec: 0.080)\n",
            "Epoch [96/100] Batch 200/333 D_loss: 0.755  G_loss: 4.008  (Adv: 2.816, Cls: 0.376, Rec: 0.082)\n",
            "Epoch [96/100] Batch 250/333 D_loss: 1.192  G_loss: 4.512  (Adv: 2.537, Cls: 1.212, Rec: 0.076)\n",
            "Epoch [96/100] Batch 300/333 D_loss: 0.566  G_loss: 3.908  (Adv: 2.778, Cls: 0.458, Rec: 0.067)\n",
            "Epoch [97/100] Batch 50/333 D_loss: 0.574  G_loss: 3.841  (Adv: 2.933, Cls: 0.197, Rec: 0.071)\n",
            "Epoch [97/100] Batch 100/333 D_loss: 0.586  G_loss: 3.867  (Adv: 2.905, Cls: 0.268, Rec: 0.069)\n",
            "Epoch [97/100] Batch 150/333 D_loss: 1.328  G_loss: 6.618  (Adv: 5.652, Cls: 0.161, Rec: 0.081)\n",
            "Epoch [97/100] Batch 200/333 D_loss: 0.672  G_loss: 3.469  (Adv: 2.467, Cls: 0.300, Rec: 0.070)\n",
            "Epoch [97/100] Batch 250/333 D_loss: 0.856  G_loss: 3.387  (Adv: 2.395, Cls: 0.228, Rec: 0.076)\n",
            "Epoch [97/100] Batch 300/333 D_loss: 1.056  G_loss: 3.689  (Adv: 2.722, Cls: 0.250, Rec: 0.072)\n",
            "Epoch [98/100] Batch 50/333 D_loss: 0.676  G_loss: 3.077  (Adv: 2.016, Cls: 0.316, Rec: 0.075)\n",
            "Epoch [98/100] Batch 100/333 D_loss: 0.742  G_loss: 5.202  (Adv: 4.251, Cls: 0.290, Rec: 0.066)\n",
            "Epoch [98/100] Batch 150/333 D_loss: 0.620  G_loss: 3.891  (Adv: 2.502, Cls: 0.588, Rec: 0.080)\n",
            "Epoch [98/100] Batch 200/333 D_loss: 1.113  G_loss: 2.813  (Adv: 1.409, Cls: 0.680, Rec: 0.072)\n",
            "Epoch [98/100] Batch 250/333 D_loss: 0.760  G_loss: 4.226  (Adv: 2.901, Cls: 0.536, Rec: 0.079)\n",
            "Epoch [98/100] Batch 300/333 D_loss: 0.570  G_loss: 4.280  (Adv: 2.860, Cls: 0.707, Rec: 0.071)\n",
            "Epoch [99/100] Batch 50/333 D_loss: 1.618  G_loss: 2.841  (Adv: 1.743, Cls: 0.348, Rec: 0.075)\n",
            "Epoch [99/100] Batch 100/333 D_loss: 0.863  G_loss: 3.405  (Adv: 2.346, Cls: 0.299, Rec: 0.076)\n",
            "Epoch [99/100] Batch 150/333 D_loss: 0.900  G_loss: 2.385  (Adv: 1.409, Cls: 0.164, Rec: 0.081)\n",
            "Epoch [99/100] Batch 200/333 D_loss: 0.675  G_loss: 3.419  (Adv: 2.596, Cls: 0.065, Rec: 0.076)\n",
            "Epoch [99/100] Batch 250/333 D_loss: 0.760  G_loss: 3.691  (Adv: 2.784, Cls: 0.257, Rec: 0.065)\n",
            "Epoch [99/100] Batch 300/333 D_loss: 1.558  G_loss: 2.992  (Adv: 1.348, Cls: 0.951, Rec: 0.069)\n",
            "Epoch [100/100] Batch 50/333 D_loss: 1.170  G_loss: 5.230  (Adv: 4.307, Cls: 0.197, Rec: 0.073)\n",
            "Epoch [100/100] Batch 100/333 D_loss: 0.838  G_loss: 3.888  (Adv: 2.770, Cls: 0.400, Rec: 0.072)\n",
            "Epoch [100/100] Batch 150/333 D_loss: 0.556  G_loss: 3.924  (Adv: 2.458, Cls: 0.747, Rec: 0.072)\n",
            "Epoch [100/100] Batch 200/333 D_loss: 1.154  G_loss: 5.783  (Adv: 5.091, Cls: 0.028, Rec: 0.067)\n",
            "Epoch [100/100] Batch 250/333 D_loss: 1.350  G_loss: 2.771  (Adv: 1.606, Cls: 0.429, Rec: 0.074)\n",
            "Epoch [100/100] Batch 300/333 D_loss: 0.637  G_loss: 3.559  (Adv: 2.386, Cls: 0.469, Rec: 0.070)\n",
            "Saved checkpoints at epoch 100\n",
            "Saved sample comparison image at epoch 100\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing configuration\n",
        "test_image_path = \"/test_10_year.jpg\"\n",
        "target_age_group = 6\n",
        "pretrained_model_path = \"/content/drive/MyDrive/UTKFace_Outputs/generator_epoch_65.pth\""
      ],
      "metadata": {
        "id": "qVZb3WgJcUT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on a Single Image\n",
        "\n",
        "if test_image_path is not None:\n",
        "    G_model = Generator(img_channels=3, label_dim=10, feature_dim=64).to(device)\n",
        "    if pretrained_model_path is not None:\n",
        "        G_model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
        "    else:\n",
        "        G_model.load_state_dict(G.state_dict())\n",
        "    G_model.eval()\n",
        "\n",
        "    test_img = Image.open(test_image_path).convert('RGB')\n",
        "\n",
        "    test_img = test_img.resize((128, 128), Image.BICUBIC)\n",
        "\n",
        "    test_img_tensor = transform_ops(test_img).unsqueeze(0).to(device)\n",
        "\n",
        "    tgt_group = max(0, min(9, target_age_group))\n",
        "    tgt_label = F.one_hot(torch.tensor([tgt_group], device=device), num_classes=10).float()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_face = G_model(test_img_tensor, tgt_label)\n",
        "\n",
        "    out_path = \"transformed_face1.png\"\n",
        "    vutils.save_image(output_face, out_path, normalize=True, value_range=(-1, 1))\n",
        "    print(f\"Single image transformed. Saved result to {out_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejELNfxovdML",
        "outputId": "3b844bbb-02a8-4cb1-f8a1-2b26e73a92fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image transformed. Saved result to transformed_face1.png\n"
          ]
        }
      ]
    }
  ]
}